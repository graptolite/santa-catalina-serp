{"cells":[{"cell_type":"markdown","id":"3f88f64c-dc46-4a20-a33a-167c59e9cf02","metadata":{},"source":"Reflector Area Processing\n=========================\n\n"},{"cell_type":"markdown","id":"0d40074d-f368-4d4f-ad49-09bdbeb5c7f1","metadata":{},"source":["To test the effect of different degrees of gap filling, this notebook was run with `dilate_erode` set to each of [0,10,20] (where 0, None and False are functionally the same). The results of these are displayed statically in the notebook in tables - however, this requires the notebook to be run with `dilate_erode` set to each of 0, 10 and 20 beforehand.\n\n"]},{"cell_type":"code","execution_count":1,"id":"6fd325b0-8040-44d2-ab42-139a75316caf","metadata":{},"outputs":[],"source":["dilate_erode = False\n# Whether to overwrite pre-existing data or not.\nFORCE_OVERWRITE = False\n\n####################################\n# Don't change anything below here #\n####################################\nif dilate_erode:\n    file_prepend = f\"modified-{dilate_erode}\"\nelse:\n    file_prepend = \"unmodified\"\n\n# General util funcs as detailed in ../section_scans-example/working.org (or its derivatives)\nfrom util_funcs import *\nfrom plotting import *\n\nimport os\ndef save_figure(path):\n    ''' Check if an image containing figure output already exists, otherwise save that figure.\n\n    path (string) : path to save the figure to.\n\n    returns None\n    '''\n    if not os.path.exists(path) or FORCE_OVERWRITE:\n        plt.savefig(path,bbox_inches=\"tight\")\n    return\n\nimport pandas as pd\nimport json\n\n# Degree of alteration assigned to each section.\nalteration_degree = {\"M04\":0,\n                     \"07A\":0,\n                     \"M08\":0,\n                     \"06C\":1,\n                     \"M07B1\":1,\n                     \"M07B2\":1,\n                     \"M01\":1,\n                     \"M02\":2}\n\n# Text descriptions for each level of alteration.\nalteration_desc = {0:\"Partly\",\n                   1:\"Heavily\",\n                   2:\"V. Heavily\"}\n\nPLT = Plotter(alteration_degree,alteration_desc)\nplot_all = PLT.plot_all"]},{"cell_type":"markdown","id":"14c3a246-77ea-4a79-8f48-bd8fbcaa1436","metadata":{},"source":["## Area Processing\n\n"]},{"cell_type":"markdown","id":"49ce637a-e67f-48ba-8253-1672e0eb4d69","metadata":{},"source":["Using methods detailed in `../section_scans-example/working.org` (or its derivatives), captured by the class `AreaProcessor` in `area_processing.py`\n\nExtracting area contours into .npy files and extracting areas into `areas.json`.\n\n-   `areas.json` is a very large file without linebreaks and so shouldn't be opened with text editors.\n\nAs an update/difference to the example processing notebook, the largest grain is also added to the data in `areas.json`.\n\n"]},{"cell_type":"code","execution_count":1,"id":"399061b7-e0e0-4020-b7ee-2bcb914d0f6c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["from area_processing import *\n\npix2mm = 1/1000\n\n# Folder containing thresholded samples (stored as .png).\nsamples_dir = os.path.join(\"..\",\"..\",\"DATASETS\",\"RL_scans\")\n\n# Load thresholded sample filenames from the folder.\nsamples = [f for f in os.listdir(samples_dir) if f.endswith(\".png\")]\n\n# Check if the areas datafile needs to be regenerated on the basis of missing file or request.\n# The areas datafile is specific to the processing pathway used to compute the areas (in terms of how much dilation-erosion is applied).\nif not os.path.exists(file_prepend + \"-areas.json\") or FORCE_OVERWRITE:\n    # Declare dictionary in which areas data will be stored.\n    areas_data = dict()\n    print(\"(Re)Generating areas.json ...\")\n    # Check if the folder for storing filtered data in is present, and if not, create this folder.\n    filtered_data_dir = \"filtered_data\"\n    if not os.path.exists(filtered_data_dir):\n        os.mkdir(filtered_data_dir)\n    # Iterate through the samples with thresholded reflectors as identified above.\n    for sample in samples:\n        print(f\"Looking at {sample}\")\n        # Initiate area processor for the active sample, conversion pixels to mm conversion factor and desired processing pathway.\n        AP = AreaProcessor(os.path.join(samples_dir,sample),pix2mm,dilate_erode)\n        # Retrieve contours.\n        contours,larger_contours = AP.load_contours()\n        # Retrieve patch areas.\n        patch_areas,units = AP.find_areas()\n        # Find the largest grain area before filtering.\n        largest_grain = max(patch_areas)\n        # Size filtering (selecting only areas larger than 5 pixels, and smaller than 0.05 mm2).\n        min_reflector_area = 5 * pix2mm**2 # mm2\n        max_reflector_area = 0.05 # mm2\n        # Construct boolean filter based on grain size.\n        size_filter = construct_minmax_filter(patch_areas,min_reflector_area,max_reflector_area)\n        # Filter the patch areas using this boolean filter.\n        patch_areas = patch_areas[size_filter]\n        # Filter \"small\" and \"large\" contours using this boolean filter.\n        contours = list_of_list_filter(contours,size_filter)\n        larger_contours = list_of_list_filter(larger_contours,size_filter)\n        # Save the filtered contours if their savefiles aren't already present.\n        base_data_file = os.path.join(filtered_data_dir,f\"{file_prepend}-{sample}\")\n        if not os.path.exists(base_data_file + \".npy\"):\n            np.save(base_data_file + \".npy\",np.array(contours,dtype=object))\n            np.save(base_data_file + \"-larger.npy\",np.array(larger_contours,dtype=object))\n\n        # Extract sample name from sample filename.\n        sample = sample.replace(\".png\",\"\")\n        # Construct dictionary to place sample-specific area data.\n        areas_data[sample] = dict()\n        # Add reflector patch areas.\n        areas_data[sample][\"patch_areas\"] = list(patch_areas)\n        # Add the area considered when looking at patch areas.\n        areas_data[sample][\"area_studied\"] = AP.area_studied()\n        # Add the largest grain observed.\n        areas_data[sample][\"largest_grain\"] = largest_grain\n    # Save all samples' areas data for this processing pathway.\n    with open(file_prepend + \"-areas.json\",\"w\") as outfile:\n        json.dump(areas_data,outfile)\nelse:\n    print(f\"Loading {file_prepend}-areas.json\")\n    # Load data from persistent storage.\n    with open(file_prepend + \"-areas.json\") as infile:\n        areas_data = json.load(infile)\nprint(\"... complete\")"]},{"cell_type":"markdown","id":"9f7c2f1b-2eff-4f68-9425-5b58329d20e5","metadata":{},"source":["### Area Distribution Plotting\n\n"]},{"cell_type":"markdown","id":"2158a4b4-ebed-48fc-ae13-51564e27e2ca","metadata":{},"source":["On the plots, the area range (x-axis) is hardcoded (to between 0 and 0.05 mm<sup>2</sup>).\n\n"]},{"cell_type":"code","execution_count":1,"id":"ee21b60a-9e92-4616-859c-b318d357c9b2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["fig = plot_all(PLT.area_distros,file_prepend)\nfig.suptitle(\"Area Distributions\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-area-distro.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"dab8cf4b-588f-4f3b-960b-aa0acad2b8d3","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-area-distro.png\"></th>\n<th><img src=\"./imgs/modified-10-area-distro.png\"></th>\n<th><img src=\"./imgs/modified-20-area-distro.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"4f6a87bf-1e4d-4a66-9a68-52cd79274aeb","metadata":{},"source":["#### Discussion\n\n"]},{"cell_type":"markdown","id":"5edaa5d9-6c46-4dec-91b7-2ee0539c92dc","metadata":{},"source":["<span class=\"timestamp-wrapper\"><span class=\"timestamp\">&lt;mer. d√©c.  6 2023 15:04&gt;</span></span>\nObservations:\n\n-   The main difference between partially and heavily altered is that the heavily altered distributions appear to overall have broader distributions.\n-   Increasing dilation-erosionn appears to broaden the distributions.\n    -   This effect is particularly pronounced for M02, a very (?) heavily altered sample.\n\nInterpretations\n\n-   Increased alteration increases growth of reflectors, biasing them towards larger sizes.\n-   Dilation-erosion causes joining of grains that don't get separated by erosion, and hence a general increase in size. This effect should be amplified if there are lots of reflector grain clusters. Therefore M02 likely has a lot of reflector grain clusters.\n\n"]},{"cell_type":"markdown","id":"92133060-3e48-4e44-9ec3-6fce9a3b435a","metadata":{},"source":["## Reflector Area vs Nearest Neighbour Distance\n\n"]},{"cell_type":"markdown","id":"a40b683b-0d77-4a87-83ba-a781725631a3","metadata":{},"source":["On the plots, the area range (x-axis) is hardcoded (to between 0 and 0.05 mm<sup>2</sup>), and the nearest neighbour distance is hardcoded (to between 0 and 1 mm).\n\n"]},{"cell_type":"code","execution_count":1,"id":"abbc8136-08bb-4cf3-b769-446b47e48a8a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["fig = plot_all(PLT.area_vs_nn_dist,file_prepend)\nfig.suptitle(\"Area vs Nearest Neighbour Distance\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-area-nn-dist.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"a97137e7-6bed-4a37-ad38-1842b04b0f3a","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-area-nn-dist.png\"></th>\n<th><img src=\"./imgs/modified-10-area-nn-dist.png\"></th>\n<th><img src=\"./imgs/modified-20-area-nn-dist.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"c5d853ea-2381-4ce5-b052-ae9fea801d4f","metadata":{},"source":["### Discussion\n\n"]},{"cell_type":"markdown","id":"079dad58-d7a9-4446-a31b-d1a9145174c7","metadata":{},"source":["<span class=\"timestamp-wrapper\"><span class=\"timestamp\">&lt;mer. d√©c.  6 2023 15:06&gt;</span></span>\nObservations:\n\n-   There's a large spread of nearest-neighbour distances for the finest grains; as grains become larger, nearest-neighbour distance appears to converge to a value around 0.1-0.2 mm.\n-   Increasing dilation-erosion increases the modal separation distance (the peak in the distributions of nearest-neighbour distance).\n-   There are more larger grains with increasing dilation-erosion, which means the convergence is clearer.\n\nInterpretations:\n\n-   Increasing dilation-erosion means grains will generally grow in size, such that a lot of low-separation fine grains become merged, hence the increase in modal separation and spreading out towards larger grain sizes.\n\n"]},{"cell_type":"markdown","id":"32d261ef-d5f4-4e13-a861-79a363372afd","metadata":{},"source":["## Reflector Aspect Ratios\n\n"]},{"cell_type":"markdown","id":"299f000f-0c3c-4283-9a45-9c0069b40168","metadata":{},"source":["On the plots, the aspect ratio range (x-axis) is hardcoded (to between 0 and 20).\n\n"]},{"cell_type":"code","execution_count":1,"id":"010e1927-6791-46b6-97c1-8a7b1dd8b812","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["fig = plot_all(PLT.aspect_ratio_distros,file_prepend)\nfig.suptitle(\"Aspect Ratio Distributions\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-aspect-ratios.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"e33126e3-83ac-44a6-beb3-c62862cde1ab","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-aspect-ratios.png\"></th>\n<th><img src=\"./imgs/modified-10-aspect-ratios.png\"></th>\n<th><img src=\"./imgs/modified-20-aspect-ratios.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"439a5775-db24-4a9b-b056-823c2cc274bb","metadata":{},"source":["### Discussion\n\n"]},{"cell_type":"markdown","id":"c3f629e6-b900-470c-bf49-380bd55e9e34","metadata":{},"source":["<span class=\"timestamp-wrapper\"><span class=\"timestamp\">&lt;mer. d√©c.  6 2023 15:10&gt;</span></span>\nObservations:\n\n-   The modal aspect ratio is nearest to 1.\n-   Aspect ratios are quite variable within each collection of samples with common degrees of alteration.\n-   In some cases increasing dilation-erosion broadens the distribution of aspect ratios (07A from 10x10 px to 20x20 px), in others it narrows (07A from no dilation-erosion to 10x10 px), and in others it has little effect (M04, M07B1 with increasing dilation erosion), with this effect being inconsistent.\n\nInterpretation:\n\n-   There's probably no confident information that can be extracted from these distributions due to a lack of consistency.\n\n"]},{"cell_type":"markdown","id":"168f4aae-8220-41ed-a9be-f10b86d3e3a5","metadata":{},"source":["## Generalised Section Properties Processing\n\n"]},{"cell_type":"markdown","id":"d605cb79-3af9-4ca1-be42-94f5b2fac2ba","metadata":{},"source":["The generalised section properties (table [1](#orgc1170a9)) are section-specific (as opposed to grain-specific) properties that were initially though to be useful to compare between sections.\n\n\n| Property|Description|Units|\n|---|---|---|\n| <code>convhull</code>|area studied|mm<sup>2</sup>|\n| <code>n</code>|number of reflectors considered||\n| <code>total_area</code>|total area covered by reflectors|mm<sup>2</sup>|\n| <code>largest</code>|area of largest reflector|mm<sup>2</sup>|\n| <code>curve_fit</code>|area distribution fit parameters||\n| <code>alteration</code>|quantitative alteration degree||\n\n"]},{"cell_type":"code","execution_count":1,"id":"c8d543d5-45d3-4e96-ae25-9e2480be83f1","metadata":{},"outputs":[],"source":["# Check if the summaries datafile needs to be regenerated on the basis of missing file or request.\nif not os.path.exists(file_prepend + \"-summary.csv\") or FORCE_OVERWRITE:\n    data = dict()\n    # Iterate through samples and their area data.\n    for sample,sample_area_data in areas_data.items():\n        # Load patch areas.\n        patch_areas = sample_area_data[\"patch_areas\"]\n        # Load area studied.\n        area_studied = sample_area_data[\"area_studied\"]\n        # Load size of largest grain.\n        largest_grain = sample_area_data[\"largest_grain\"]\n        # Compute distribution parameters for patch areas.\n        # Note 99 rather than 100 as bin_values takes the number of bins rather than bin edges.\n        counts,_,midpoints = bin_values(patch_areas,0.05,99)\n\n        # Construct summary dataframe for each sample.\n        data[sample] = {\"convhull\":area_studied, # study area\n                        \"n\":len(patch_areas), # number of discrete reflectors after filtering\n                        \"total_area\":sum(patch_areas), # area of reflectors after filtering\n                        \"largest\":largest_grain, # largest continuous reflector patch area\n                        \"curve_fit\":fit_exp_log_y(midpoints,counts)}\n\n        # Degree of alteration assigned to each section.\n        # Note: alteration_degree is imported from plotting.py\n        try:\n            data[sample][\"alteration\"] = alteration_degree[sample]\n        except KeyError:\n            pass\n\n    # Convert dictionary to pandas dataframe.\n    df = pd.DataFrame.from_dict(data,orient=\"index\")\n    # Save pandas dataframe to .csv file.\n    df.to_csv(file_prepend + \"-summary.csv\")"]},{"cell_type":"markdown","id":"35f60a0c-d83e-4164-a484-0b0b6c537f75","metadata":{},"source":["### Comparison Plotting\n\n"]},{"cell_type":"markdown","id":"123ab413-32dd-431a-b513-fcf61d0e0e78","metadata":{},"source":["After obtaining this data, comparisons can be plotted.\n\n-   In some cases, derived parameters (that are normalised to the area studied) are more useful for comparing between sections.\n    -   Reflector coverage area &rarr; reflector coverage percentage.\n    -   Reflector count &rarr; reflector number density.\n-   Only sections that are partially (0) or heavily (1) altered will be considered in the comparison.\n\n"]},{"cell_type":"code","execution_count":1,"id":"869b7141-eeac-4202-a415-968a8c42b0d0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Force load from .csv file so that list processing is standardised.\ndf = pd.read_csv(file_prepend + \"-summary.csv\",index_col=0)\n# Derived parameters that are more logical to compare between sections.\ndf[\"reflector_percentage\"] = df[\"total_area\"]/df[\"convhull\"] * 100\ndf[\"number_density\"] = df[\"n\"]/df[\"convhull\"]\n\n# Look at only sections that have an alteration index of 1 (heavy) or 0 (partly).\ndf = df[(df[\"alteration\"]==1) | (df[\"alteration\"]==0)]\n\n######################################################\n# Comparison between aggregated reflector properties #\n######################################################\nfig,axs = plt.subplots(1,3,constrained_layout=True,figsize=(9,6))\n\n# Plot point for each sample's property.\naxs[0].scatter(df[\"alteration\"],df[\"largest\"])\naxs[1].scatter(df[\"alteration\"],df[\"number_density\"])\naxs[2].scatter(df[\"alteration\"],df[\"reflector_percentage\"])\n\n# Label the sample referred to by each point.\nfor s,row in df.iterrows():\n    x = row[\"alteration\"]\n    axs[0].text(x,row[\"largest\"],s)\n    axs[1].text(x,row[\"number_density\"],s)\n    axs[2].text(x,row[\"reflector_percentage\"],s)\n\n# Label the plots with which parameter is being compared.\naxs[0].set_ylabel(\"Largest reflector area /mm$^2$\")\naxs[1].set_ylabel(\"Reflector number density /mm$^-2$\")\naxs[2].set_ylabel(\"Reflector coverage /%\")\n\n# Label the plots with the degree of alteration represented by plotted samples.\n[ax.set_xlabel(\"Degree of alteration\") for ax in axs]\n[ax.set_xticks([0,1],[\"medium\",\"high\"]) for ax in axs]\n\nplt.suptitle(\"Reflector parameter comparisons between\\nmoderately and highly altered rocks\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-refl-param-comparison.png\"))\n\n#############################################\n# Comparison between area distribution fits #\n#############################################\nfig,axs = plt.subplots(1,2,constrained_layout=True,figsize=(6,6))\n\n# Load curve fit data.\ncurve_fits = np.array(json.loads(\"[\" + \",\".join(df[\"curve_fit\"]) + \"]\"))\n\n# Plot point for each sample's property.\naxs[0].scatter(df[\"alteration\"],curve_fits[:,0]/df[\"n\"])\naxs[1].scatter(df[\"alteration\"],curve_fits[:,1])\n\n# Label the plots with which parameter is being compared.\naxs[0].set_ylabel(\"a/n\")\naxs[1].set_ylabel(\"b\")\n\n# Label the sample referred to by each point.\nfor i,alt in enumerate(zip(curve_fits[:,0]/df[\"n\"],curve_fits[:,1])):\n    s = df.iloc[i].name\n    x = df.iloc[i][\"alteration\"]\n    axs[0].text(x,alt[0],s)\n    axs[1].text(x,alt[1],s)\n\n# Label the plots with the degree of alteration represented by plotted samples.\n[ax.set_xlabel(\"Degree of alteration\") for ax in axs]\n[ax.set_xticks([0,1],[\"medium\",\"high\"]) for ax in axs]\n\nplt.suptitle(\"Fit parameter values in area distribution curve fit of format: $10^{a \\cdot \\exp(b x)}$\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-area_fit_param_comp.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"ebbca30f-cb15-4e96-8017-baf6d65b2a94","metadata":{},"source":["For the area distribution curve fits, and interpretation of the parameters' meanings are:\n\n-   $a$: height of the distribution at the start such that $a/n$ is the height normalised by the number of reflectors (to permit comparison between sections). The larger $|a/n|$ is, the taller the start of the distribution relative to higher values.\n-   $b$: measure of \"decay\" rate of the negative exponential distribution. The larger $|b|$ is, the narrower the distribution.\n\n"]},{"cell_type":"markdown","id":"326eb224-a07a-4817-89c0-b0535424049f","metadata":{},"source":["#### Reflector Parameter Comparison\n\n"]},{"cell_type":"markdown","id":"b2c264b8-0e49-4680-9660-01e074452158","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-refl-param-comparison.png\"></th>\n<th><img src=\"./imgs/modified-10-refl-param-comparison.png\"></th>\n<th><img src=\"./imgs/modified-20-refl-param-comparison.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"29a2f9ff-bbcf-4d1b-bc28-578dc2d41b14","metadata":{},"source":["##### Discussion\n\n"]},{"cell_type":"markdown","id":"8fbc2676-d5ca-4013-9c75-e5c62cf2e192","metadata":{},"source":["<span class=\"timestamp-wrapper\"><span class=\"timestamp\">&lt;mer. d√©c.  6 2023 15:26&gt;</span></span>\nObservations:\n\n-   There's a narrowing of the range of values towards the smaller end for the largest parameter area with increasing alteration. This narrowing is most pronounced at 20x20 px dilation-erosion.\n-   The reflector number density appears to also broaden in range with increasing alteration. This effect is clearest without dilation erosion, and is roughly equally less clear for 10x10 and 20x20 px dilation-erosion.\n-   The reflector coverage density appears to broaden in range with increasing alteration but only clearly so at 20x20 px dilation-erosion.\n\nInterpretation:\n\n-   Due to the greater effect of heterogeneity on larger grains, the difference in largest grain sizes can't be confidently interpreted.\n-   Broadening of number density and coverage suggests that increasing alteration can either have little effect on reflector number density, or can increase it.\n-   The effect of different amounts of dilation-erosion is relatively important in determining how clear these changes in range are.\n\n"]},{"cell_type":"markdown","id":"0d8b1a4f-e533-4fe6-977c-8d4583f304f8","metadata":{},"source":["#### Area Distribution Comparison\n\n"]},{"cell_type":"markdown","id":"ed40486b-efba-4594-9cdd-cd2282529296","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-area_fit_param_comp.png\"></th>\n<th><img src=\"./imgs/modified-10-area_fit_param_comp.png\"></th>\n<th><img src=\"./imgs/modified-20-area_fit_param_comp.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"e82b0d90-5ee3-4cae-8982-3b92b675c01a","metadata":{},"source":["##### Discussion\n\n"]},{"cell_type":"markdown","id":"2e4e61e5-fedd-4c41-bc00-d914af7925aa","metadata":{},"source":["<span class=\"timestamp-wrapper\"><span class=\"timestamp\">&lt;mer. d√©c.  6 2023 15:37&gt;</span></span>\nObservations:\n\n-   $a/n$ generally broadens with increasing alteration. The amount of dilation-erosion has little effect on this.\n-   $b$ generally decreases lower magnitudes with increasing alteration, with this effect being more pronounced with increasing dilation-erosion.\n\nInterpretations:\n\n-   Increasing alteration can change the relative size of the lowest area bin in different directions.\n-   Increasing alteration generally broadens the area distribution, with this effect being more obvious with increasing dilation-erosion.\n\n"]},{"cell_type":"markdown","id":"3d173f68-5470-4ab0-9f06-332de099bccc","metadata":{},"source":["## Sample Property Aggregation\n\n"]},{"cell_type":"markdown","id":"6fbd5f1a-8fca-436d-8ab0-362c88a34a1e","metadata":{},"source":["Area distributions can be aggregated and differenced to make inferences on the grain population produced with increasing hydration.\n\nLooking at just the partially vs heavily altered sections (as the very heavily altered section just has one entry and is uncertain anyway):\n\n"]},{"cell_type":"code","execution_count":1,"id":"35ff7dc6-7138-4319-9ab2-a059a2e43e7a","metadata":{},"outputs":[],"source":["# Overwriting the imported sample list with just the samples of interest (i.e. that have alteration indices of either 0 or 1).\nalteration_degree = {k:v for k,v in alteration_degree.items() if v in [0,1]}"]},{"cell_type":"markdown","id":"7cc2f1da-59c1-4ac4-9aec-f8339f2db286","metadata":{},"source":["Loading area data and defining how it's being binned:\n\n"]},{"cell_type":"code","execution_count":1,"id":"2310febb-800d-4a24-a5c1-294beee232a4","metadata":{},"outputs":[],"source":["with open(file_prepend + \"-areas.json\") as infile:\n    data = json.load(infile)\n\n# Hardcoded maximum area to define bins with.\nmax_area = 0.05 # mm^2\nbins = np.linspace(0,max_area,100)\n# Compute bin midpoints.\nmidpoints = (bins[1:] + bins[:-1])/2\n# Function to normalise data.\nnorm = lambda x : np.array(x)/sum(x)"]},{"cell_type":"markdown","id":"d8e36a04-a893-4262-88a3-42ba599fec54","metadata":{},"source":["Grouping normalised area distributions by degree of alteration, with each distribution weighted by how much area was studied to produce the distribution.\n\n"]},{"cell_type":"code","execution_count":1,"id":"291795a7-bec4-4501-b494-a0c86ce2b3ca","metadata":{},"outputs":[],"source":["# Declare dictionary in which data will be aggregated.\ngrouped_data = dict()\n# Iterate through sample data.\nfor key,area_data in data.items():\n    # Extract areas data.\n    areas = area_data[\"patch_areas\"]\n    # Extract the area studied.\n    studied_area = area_data[\"area_studied\"]\n    # Check if the sample is of interest.\n    if key in alteration_degree:\n        # If so, extract the degree of alteration of the sample.\n        alteration = alteration_degree[key]\n        # Check if the degree of alteration of interest already has a preallocated data structure in the top-level dictionary dataframe.\n        if not alteration in grouped_data:\n            # If not, create this data structure.\n            grouped_data[alteration] = {\"distribution\":[],\n                                        \"n\":0}\n        # Compute area distribution via histogram.\n        counts,_ = np.histogram(areas,bins=bins)\n        # Normalise the distribution.\n        normed_counts = norm(counts)\n        # Weight the distribution by the amount of area studied to produce that distribution.\n        weighted_counts = studied_area * normed_counts\n        # Store the distribution.\n        grouped_data[alteration][\"distribution\"].append(weighted_counts)\n        # Add to the number of reflector patches considered for sections of the active degree of alteration.\n        grouped_data[alteration][\"n\"] += len(areas)\n\n# Aggregate and normalise the distributions.\npartially_altered = norm(np.sum(np.array(grouped_data[0][\"distribution\"]),axis=0))\nheavily_altered = norm(np.sum(np.array(grouped_data[1][\"distribution\"]),axis=0))"]},{"cell_type":"markdown","id":"589209ec-eeaf-441d-a579-56e2616731fe","metadata":{},"source":["Fitting a combined exponential and order 1 polynomial decay function to the distributions, and then saving the results of the fit to permit later investigation of the robustness of difference of distributions.\n\n"]},{"cell_type":"code","execution_count":1,"id":"a1d98f75-83f2-4dac-9a41-6b3228822a17","metadata":{},"outputs":[],"source":["# Only fit to positive values (i.e. where the count is not zero).\nfitting_p = partially_altered>0\nfitting_h = heavily_altered>0\n\n# Determine fit parameters.\npopt_p,_ = curve_fit(exp_with_first_order_p_func,\n                     midpoints[fitting_p],np.log10(partially_altered[fitting_p]))\npopt_h,_ = curve_fit(exp_with_first_order_p_func,\n                     midpoints[fitting_h],np.log10(heavily_altered[fitting_h]))\n\n# Save fit parameters.\nwith open(file_prepend + \"-distribution_fits.json\",\"w\") as outfile:\n    json.dump({\"partial\":popt_p.tolist(),\n               \"heavy\":popt_h.tolist(),\n               \"bins\":bins.tolist()},\n              outfile)"]},{"cell_type":"markdown","id":"515ced57-6a53-42cc-81f8-b25b6100f414","metadata":{},"source":["### Plotting Aggregated Distributions\n\n"]},{"cell_type":"code","execution_count":1,"id":"e34f0ac6-8705-44f9-accf-88a04af1e256","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Plot the aggregated area distribution for partially altered samples, as well as the fit.\nplt.stairs(partially_altered,bins,label=\"partially\",color=\"b\")\nplt.plot(midpoints,10**exp_with_first_order_p_func(midpoints,*popt_p),c=\"b\")\n# Plot the aggregated area distribution for heavily altered samples, as well as the fit.\nplt.stairs(heavily_altered,bins,label=\"heavily\",color=\"g\")\nplt.plot(midpoints,10**exp_with_first_order_p_func(midpoints,*popt_h),c=\"g\")\n# Set y scale to log.\nplt.gca().set_yscale(\"log\")\n# Label axes.\nplt.xlabel(\"Area /mm$^2$\")\nplt.ylabel(\"Frequency\")\n# Display legend.\nplt.legend()\n\nsave_figure(os.path.join(\"imgs\",file_prepend+\"-partially-vs-heavily-altered.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"4d7d8c68-e42a-46bb-8156-443f87f16496","metadata":{},"source":["Generally speaking, these fits are not great &#x2026;\n\n<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-partially-vs-heavily-altered.png\"></th>\n<th><img src=\"./imgs/modified-10-partially-vs-heavily-altered.png\"></th>\n<th><img src=\"./imgs/modified-20-partially-vs-heavily-altered.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"e39d2571-a37c-4e7c-b16a-f6d75bcf17e9","metadata":{},"source":["#### Discussion\n\n"]},{"cell_type":"markdown","id":"40adb201-9322-43e7-b547-b117d7d13b59","metadata":{},"source":["<span class=\"timestamp-wrapper\"><span class=\"timestamp\">&lt;mer. d√©c.  6 2023 15:48&gt;</span></span>\nObservations:\n\n-   These fits aren't great (even ignoring the semilog nature of these plots)\n\nInterpretation:\n\n-   A better fit function may be needed - or manually drawing continuous distributions?\n\n"]},{"cell_type":"markdown","id":"dc7595b8-cbbf-40b5-8be9-8ae0c4ea8de3","metadata":{},"source":["### Plotting Differenced Distributions\n\n"]},{"cell_type":"markdown","id":"0ada2a9f-bd23-4ba4-bc9f-326d9f4aa776","metadata":{},"source":["Plotting the difference in heavily altered distribution and partially altered distribution to characterise the change following increasing alteration.\n\n"]},{"cell_type":"code","execution_count":1,"id":"d3147084-1d1d-470d-a241-3817b10d4927","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Compute difference in distributions.\ndiff = heavily_altered-partially_altered\n# Plot horizontal line at y=0.\nplt.axhline(0,c=\"lightblue\",linestyle=\"--\")\n# Plot difference in distributions\nplt.stairs(diff,bins,label=\"heavily-partially altered freqs.\",color=\"k\")\n# Label axes.\nplt.xlabel(\"Area /mm$^2$\")\nplt.ylabel(\"Heavily minus Partially altered Freq. Diff.\")\n\nsave_figure(os.path.join(\"imgs\",file_prepend+\"-heavily-minus-partially-altered.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"3cb165f7-8d56-4c52-84f7-721b03c4811d","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-heavily-minus-partially-altered.png\"></th>\n<th><img src=\"./imgs/modified-10-heavily-minus-partially-altered.png\"></th>\n<th><img src=\"./imgs/modified-20-heavily-minus-partially-altered.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"badda952-f395-43b5-99a7-1a5ccb34319d","metadata":{},"source":["#### Discussion\n\n"]},{"cell_type":"markdown","id":"df77a3ed-c058-496e-b5fb-271f6e87fd20","metadata":{},"source":["<span class=\"timestamp-wrapper\"><span class=\"timestamp\">&lt;mer. d√©c.  6 2023 15:49&gt;</span></span>\nObservations:\n\n-   There's a relatively consistent observation across the range of dilation-erosions tested that there's a decrease in the proportion of some finer grain size, with an increase in grains just coarser, and that increase decaying with increasing grain size up to ~0.01 mm<sup>2</sup>.\n    -   Where no dilation-erosion is applied, there's an increase in the finest grain size fraction considered, but when 10x10 or 20x20 px dilation-erosion is applied, the finest fraction experiences a significant decrease.\n\nInterpretations:\n\n-   The finest grains are dissolved and reprecipitated on other smaller grains to increase their grain size.\n    -   This is different to the finest grains being preferentially dissolved as that would increase the proportion of all coarser grains.\n    -   I.e. the decaying increase in grain proportions between ~0.002 and 0.01 mm<sup>6</sup> is likely related to precipitation effects.\n\n"]},{"cell_type":"markdown","id":"14dfef20-831e-459d-80f2-73b9b333ba6c","metadata":{},"source":["### Testing the Robustness of the Difference in Distributions\n\n"]},{"cell_type":"markdown","id":"ccb15d89-9dd8-4ac8-87c4-b57aba7d1d9b","metadata":{},"source":["The difference is distributions can be tested by a bootstrapping method assuming the fitted distributions accurately reflect the parent distribution of the observed distributions:\n\n1.  Generate a subsample of each fitted distribution, where the size of each subsample corresponds to the size of the dataset used for each aggregated distribution.\n2.  Use some statistical method of determining whether two empirical (i.e. discrete) distributions are different to calculate the confidence with this the distributions can be described as different. Some methods identified are described in table [1](#org6960067). Based on my interpretation of these tests, the Kolmogorov-Smirnov test should be the most useful in quantifying whether the distributions are likely different or not. However it's scipy implementation is only valid for continuous distributions *sensu stricto*.\n3.  Repeat this process of subsampling and comparing many times until a relatively smooth distribution of confidences is produced.\n\n\n| Method|H0 (for two samples)|\n|---|---|\n| Student T Test|Averages don't differ|\n| (2 Sample) Kolmogorov-Smirnov Test|Distributions don't differ (both in location and shape)|\n| Mann-Whitney U Test|There's an equal probability of a randomly selected value from one distribution being greater than vs less than a randomly selected value from the other distribution|\n\n"]},{"cell_type":"code","execution_count":1,"id":"64e47323-1b2c-4347-b03b-609d6b6b95cd","metadata":{},"outputs":[],"source":["def bootstrap_test_difference(difference_test,test_repeats,size1,p1,size2,p2):\n    # Allocate lists into which test outcomes are to be saved.\n    difference_test_results_11 = []\n    difference_test_results_12 = []\n\n    # Repeat the analysis the desired number of times.\n    for i in range(test_repeats):\n        # Random sample based on the first distribution of the first sample's size.\n        rand_a = rng.choice(midpoints,size=size1,p=norm(p1))\n        # Random sample based on the first distribution of the second sample's size.\n        rand_b = rng.choice(midpoints,size=size2,p=norm(p1))\n        # Random sample based on the second distribution of the second sample's size.\n        rand_c = rng.choice(midpoints,size=size2,p=norm(p2))\n        # Performing the difference test for random samples extracted from the same distribution.\n        difference_test_results_11.append(list(difference_test(rand_a,rand_b)))\n        # Performing the difference test for random samples extracted from different distributions.\n        difference_test_results_12.append(list(difference_test(rand_a,rand_c)))\n\n    # Convert test outcome lists into numpy arrays.\n    difference_test_results_11 = np.array(difference_test_results_11)\n    difference_test_results_12 = np.array(difference_test_results_12)\n    return difference_test_results_11,difference_test_results_12"]},{"cell_type":"markdown","id":"efcaf910-7cc0-4744-b077-353c2876d03a","metadata":{},"source":["Scipy tests generate a test statistic, as well as a P-value. The null hypotheses for the various tests in the table effectively boil down to \"the distributions are the same\". In order to reject this null hypothesis, the observed P-value must be below a critical value - this is taken (somewhat arbitrarily) at 0.05. Therefore, in the distribution of P-values (generated by bootstrapping), the percentage of P-values below 0.05 is a proxy for the likelihood that the two distributions (derived from data of almost-certainly unequal sizes) are different.\n\n-   To avoid this taking an excessively long time, an undersampling factor is introduced, which reduces the size of both samples by a common factor (through division by the undersampling factor).\n-   The larger this undersampling factor is, the greater the effect of randomness on the distributions. However, the fundamental nature of the test should remain.\n-   Regardless, keeping the undersampling factor as close to 1 as feasible is preferred.\n\n"]},{"cell_type":"code","execution_count":1,"id":"25945e53-c804-4e93-9ba8-a6fd0e74f0ab","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# For distribution testing functions.\nimport scipy.stats as sps\n\n#####\n\ntest_repeats = 5000\np_crit = 0.05\n\n#####\n\n# Factor by which to reduce the number of items in each sample.\nundersample_factor = 20\n# Number of items in each sample based on how many were present in the data.\nn_partly_altered = int(grouped_data[0][\"n\"]/undersample_factor)\nn_heavily_altered = int(grouped_data[1][\"n\"]/undersample_factor)\n\nprint(f\"Number in first sample: {n_partly_altered}\\nNumber in second sample: {n_heavily_altered}\")\n\n# Declare the types of statistical difference tests to apply.\ndifference_tests = [sps.ttest_ind,sps.ks_2samp,sps.mannwhitneyu]\n\n# Load the distribution fits data.\nwith open(file_prepend + \"-distribution_fits.json\") as infile:\n    data = json.load(infile)\n# Extract fit parameters.\nfit_p = data[\"partial\"]\nfit_h = data[\"heavy\"]\n# Extract bins used to produce the distributions that the fit parameters were derived from.\nbins = np.array(data[\"bins\"])\n# Compute the midpoints of these bins.\nmidpoints = (bins[:-1] + bins[1:])/2\n\n# Function to construct a discrete probability distribution (for specified x values) applicable to the fit function used to produce the fit parameters above.\np_x = lambda x,fit : 10**exp_with_first_order_p_func(x,*fit)\n# Various partial functions:\np_x_p = lambda x : p_x(x,fit1)\np_x_h = lambda x : p_x(x,fit2)\np = lambda fit : p_x(midpoints,fit)\n# Discrete probability distributions.\np_p = p(fit_p)\np_h = p(fit_h)\n\n# Function to normalise data.\nnorm = lambda x : x/sum(x)\n\n# Initiate random number generator.\nrng = np.random.default_rng()\n\n# Specify top-level plot layout.\nfig,axs_0 = plt.subplots(2,len(difference_tests),constrained_layout=True,figsize=(20,20))\n\n# Iterate through the statistical difference tests.\nfor i,difference_test in enumerate(difference_tests):\n    # Obtain bootstrapped results for the active difference test.\n    difference_test_results_11,difference_test_results_12 = bootstrap_test_difference(difference_test,test_repeats,n_partly_altered,p_p,n_heavily_altered,p_h)\n\n    # Define number bins for the distribution of p-values.\n    p_val_bins = np.linspace(0,1,101)\n\n    # Extract p-values for the results of different combination of distributions (same distribution vs different distributions).\n    p_val11 = difference_test_results_11[:,1]\n    p_val12 = difference_test_results_12[:,1]\n\n    # Isolate a column in the plot.\n    axs = axs_0[:,i]\n    # Iterate through the p-values for each combination of distributions.\n    for p_vals,ax in zip([p_val11,p_val12],axs):\n        # Plot the distribution for each collection of p-values.\n        ax.hist(p_vals,p_val_bins)\n        # Plot a vertical line at the critical p-value.\n        ax.axvline(p_crit,color=\"r\")\n        # Label axes.\n        ax.set_xlabel(\"p value\")\n        ax.set_ylabel(\"Count\")\n        # Compute and display the percentage of observed p-values below critical.\n        ax.set_title(\"%s; p<%s = %.2f%%\" % (difference_test.__name__,p_crit,sum(p_vals<p_crit)/len(p_vals)*100))\n\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-diff-test.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"442b86c0-f02c-42fb-bc43-16ef3f65d63a","metadata":{},"source":["In the following figures, the top row contains the test results from samples drawn from the same distribution (the partially altered distribution), and the bottom row from different distributions (partially altered vs heavily altered).\n\n<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-diff-test.png\"></th>\n<th><img src=\"./imgs/modified-10-diff-test.png\"></th>\n<th><img src=\"./imgs/modified-20-diff-test.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"1e7a0e41-665b-4d93-a034-3fd0c720657c","metadata":{},"source":["#### Discussion\n\n"]},{"cell_type":"markdown","id":"d431fbd8-cc57-4a1e-831e-b2d92cb3239e","metadata":{},"source":["<span class=\"timestamp-wrapper\"><span class=\"timestamp\">&lt;mer. d√©c.  6 2023 15:54&gt;</span></span>\nObservations:\n\n-   Out of all the tests, only the KS-test is able to clearly identify the same distributions as being the same - i.e. peak in p-value distributions at high p (the other tests appear to produce an even distribution of p-values, which suggests the same distributions are equally as likely to be the same as to being different).\n    -   Therefore, the results of the KS-test are probably the most reliable in this circumstance.\n-   All tests seem to suggest that the partially vs heavily altered distributions are different for all amounts of dilation-erosion.\n    -   However, the t-test seems to be quite sensitive to differences in fit shapes produced by different amounts of dilation-erosion.\n\nInterpretations:\n\n-   Subsamples of the fitted continuous distributions of the partially vs heavily altered empirical area distributions have a high likelihood of being different. Therefore - assuming the empirical area distributions represent subsamples of their respective fitted continuous distributions - the empirical area distributions are likely different as well.\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}
