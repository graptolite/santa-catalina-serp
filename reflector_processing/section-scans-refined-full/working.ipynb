{"cells":[{"cell_type":"markdown","id":"7d9a5f70-56fe-473b-9533-0fb0ecb27a29","metadata":{},"source":"Reflector Area Processing - Refined Method\n==========================================\n\n"},{"cell_type":"markdown","id":"948fc698-f8de-4589-896a-13e27dd77860","metadata":{},"source":["To test the effect of different degrees of gap filling, this notebook was run with dilate erode set to each of [0,10,20] (where 0, None and False are functionally the same). The results of these are displayed statically in the notebook in tables - however, this requires the notebook to be run with `dilate_erode` set to each of 0, 10 and 20 beforehand.\n\n"]},{"cell_type":"code","execution_count":1,"id":"3e7d001f-06e1-4ae5-b859-61595a0a3234","metadata":{},"outputs":[],"source":["FORCE_OVERWRITE = False\ndilate_erode = 10\n\n####################################\n# Don't change anything below here #\n####################################\nif dilate_erode:\n    file_prepend = f\"modified-{dilate_erode}\"\nelse:\n    file_prepend = \"unmodified\"\n\nimport sys\nimport os\nsys.path.insert(0,os.path.join(\"..\",\"section-scans-full\"))\n# General util funcs as detailed in ../section_scans-runthrough-example/working.org (or its derivatives)\nfrom util_funcs import *\nfrom plotting import *\n\ndef save_figure(path):\n    ''' Check if an image containing figure output already exists, otherwise save that figure.\n\n    path (string) : path to save the figure to.\n\n    returns None\n    '''\n    if not os.path.exists(path) or FORCE_OVERWRITE:\n        plt.savefig(path,bbox_inches=\"tight\")\n    return\n\nimport pandas as pd\nimport json\n\n# Degree of alteration assigned to each section.\nalteration_degree = {\"M04\":0,\n                     \"07A\":0,\n                     \"M08\":0,\n                     \"06C\":1,\n                     \"M07B1\":1,\n                     \"M07B2\":1,\n                     \"M01\":1,\n                     \"M02\":2}\n\n# Text descriptions for each level of alteration.\nalteration_desc = {0:\"Partly\",\n                   1:\"Heavily\",\n                   2:\"V. Heavily\"}\n\nPLT = Plotter(alteration_degree,alteration_desc)\nplot_all = PLT.plot_all\n\n# Make sure imgs folder exists.\nif not os.path.exists(\"imgs\"):\n    os.mkdir(\"imgs\")"]},{"cell_type":"markdown","id":"58dd4991-ee77-414c-8873-df53840337ff","metadata":{},"source":["## Area Processing\n\n"]},{"cell_type":"markdown","id":"a75b886e-de20-47ae-91a9-af11f7117981","metadata":{},"source":["Using methods detailed in `../section_scans-example/working.org` (or its derivatives), with refinement detailed in `../section_scans-validation/working.org` (or its derivatives), captured by the class `AreaProcessor` in `updated_area_processing.py`\n\nExtracting area contours into .npy files and extracting areas into `areas.json`.\n\n-   `areas.json` is a very large file without linebreaks and so shouldn't be opened with text editors.\n\nAs an update/difference to the example processing notebook, the largest grain is also added to the data in `areas.json`.\n\n"]},{"cell_type":"code","execution_count":1,"id":"06ca9852-1f6e-4ed2-8a03-6e52ea130988","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["from updated_area_processing import *\n\npix2mm = 1/1000\n\n# Folder containing thresholded samples (stored as .png).\n# Note: folder linking has been used - this is not the real path.\nsamples_dir = os.path.join(\"..\",\"..\",\"DATASETS\",\"RL_scans\")\n\n# Load thresholded sample filenames from the folder.\nsamples = [f for f in os.listdir(samples_dir) if f.endswith(\".png\")]\n\n# Check if the areas datafile needs to be regenerated on the basis of missing file or request.\n# The areas datafile is specific to the processing pathway used to compute the areas (in terms of how much dilation-erosion is applied).\nif not os.path.exists(file_prepend + \"-areas.json\") or FORCE_OVERWRITE:\n    # Declare dictionary in which areas data will be stored.\n    areas_data = dict()\n    print(\"(Re)Generating areas.json ...\")\n    # Iterate through the samples with thresholded reflectors as identified above.\n    for sample in samples:\n        print(f\"Looking at {sample}\")\n        # Initiate area processor for the active sample, conversion pixels to mm conversion factor and desired processing pathway.\n        AP = AreaProcessor(os.path.join(samples_dir,sample),pix2mm,dilate_erode)\n        # Retrieve contours.\n        contours,larger_contours = AP.load_contours()\n        # Retrieve patch areas.\n        patch_areas,units = AP.find_areas()\n\n        # Find the largest grain area before filtering.\n        largest_grain = max(patch_areas)\n\n        # Size filtering (selecting only areas smaller than 0.05 mm2; areas are already filtered to greater than 5 px by AP).\n        max_reflector_area = 0.05 # mm2\n\n        # Construct boolean filter based on grain size.\n        size_filter = construct_minmax_filter(patch_areas,None,max_reflector_area)\n        # Filter the patch areas using this boolean filter.\n        patch_areas = patch_areas[size_filter]\n\n        # Filter \"small\" and \"large\" contours using this boolean filter.\n        contours = list_of_list_filter(contours,size_filter)\n        larger_contours = list_of_list_filter(larger_contours,size_filter)\n\n        # Check if the folder for storing filtered data in is present, and if not, create this folder.\n        filtered_data_dir = \"filtered_data\"\n        if not os.path.exists(filtered_data_dir):\n            os.mkdir(filtered_data_dir)\n        # Save the filtered contours if their savefiles aren't already present.\n        base_data_file = os.path.join(filtered_data_dir,f\"{file_prepend}-{sample}\")\n        if not os.path.exists(base_data_file + \".npy\"):\n            np.save(base_data_file + \".npy\",np.array(contours,dtype=object))\n            np.save(base_data_file + \"-larger.npy\",np.array(larger_contours,dtype=object))\n\n        # Extract sample name from sample filename.\n        sample = sample.replace(\".png\",\"\")\n        # Construct dictionary to place sample-specific area data.\n        areas_data[sample] = dict()\n        # Add reflector patch areas.\n        areas_data[sample][\"patch_areas\"] = list(patch_areas)\n        # Add the area considered when looking at patch areas.\n        areas_data[sample][\"area_studied\"] = AP.area_studied()\n        # Add the largest grain observed.\n        areas_data[sample][\"largest_grain\"] = largest_grain\n    # Save all samples' areas data for this processing pathway.\n    with open(file_prepend + \"-areas.json\",\"w\") as outfile:\n        json.dump(areas_data,outfile)\nelse:\n    print(f\"Loading {file_prepend}-areas.json\")\n    # Load data from persistent storage.\n    with open(file_prepend + \"-areas.json\") as infile:\n        areas_data = json.load(infile)\nprint(\"... complete\")"]},{"cell_type":"markdown","id":"e62be5de-f8d6-4555-ba7d-81a1457230a0","metadata":{},"source":["### Area Distribution Plotting\n\n"]},{"cell_type":"markdown","id":"2dcdea4b-fd96-40db-9b6a-71562e7863a4","metadata":{},"source":["On the plots, the area range (x-axis) is hardcoded (to between 0 and 0.05 mm<sup>2</sup>).\n\n"]},{"cell_type":"code","execution_count":1,"id":"02cd7a83-d6a0-43c3-b388-50e80786c175","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["fig = plot_all(PLT.area_distros,file_prepend)\nfig.suptitle(\"Area Distributions\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-area-distro.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"dff80f31-d38b-428a-ab1f-d7924d5f7479","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-area-distro.png\"></th>\n<th><img src=\"./imgs/modified-10-area-distro.png\"></th>\n<th><img src=\"./imgs/modified-20-area-distro.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"03d8529a-d2aa-47eb-b164-db75daa6190e","metadata":{},"source":["#### Discussion\n\n"]},{"cell_type":"markdown","id":"1dd04e2f-fd6e-4dd9-bde2-1176f2bb3c88","metadata":{},"source":["Observations:\n\n-   The main difference between partially and heavily altered is that the heavily altered distributions appear to overall have broader distributions. However, the difference doesn't seem as obvious at higher degrees of dilation-erosion than before.\n-   Increasing dilation-erosion appears to broaden the distributions. However, this effect seems to be much less pronounced than before.\n\nInterpretations\n\n-   Increased alteration increases growth of reflectors, biasing them towards larger sizes.\n-   Dilation-erosion causes joining of grains that don't get separated by erosion, and hence a general increase in size. Since this effect was actually amplified more so be the joining/bridging over sub-5 px grains in the original method, the removal of these sub-5 px grains has reduced this issue\n    -   The fact that this effect is much less pronouced provides further evidence for the robustness of the new method in recovering actual signals.\n\n"]},{"cell_type":"markdown","id":"b6928e6c-e6ff-4f50-9f82-588f645eaff4","metadata":{},"source":["## Reflector Area vs Nearest Neighbour Distance\n\n"]},{"cell_type":"markdown","id":"f651dd69-3a7a-4f96-b7ea-0e84113b5a86","metadata":{},"source":["On the plots, the area range (x-axis) is hardcoded (to between 0 and 0.05 mm<sup>2</sup>), and the nearest neighbour distance is hardcoded (to between 0 and 1 mm).\n\n"]},{"cell_type":"code","execution_count":1,"id":"1240867e-fbab-4d56-afe4-bfd01149d68d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["fig = plot_all(PLT.area_vs_nn_dist,file_prepend)\nfig.suptitle(\"Area vs Nearest Neighbour Distance\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-area-nn-dist.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"aa067746-9964-4a13-9b45-784cacc37586","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-area-nn-dist.png\"></th>\n<th><img src=\"./imgs/modified-10-area-nn-dist.png\"></th>\n<th><img src=\"./imgs/modified-20-area-nn-dist.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"bcc3b403-abd1-4a5d-b2f0-7453d9f95b6d","metadata":{},"source":["### Discussion\n\n"]},{"cell_type":"markdown","id":"64325f9c-e4c9-4655-9e74-7e71b93d86b1","metadata":{},"source":["Observations:\n\n-   There's a large spread of nearest-neighbour distances for the finest grains; as grains become larger, nearest-neighbour distance appears to converge to a value around 0.2 mm.\n-   Increasing dilation-erosion increases the modal separation distance (the peak in the distributions of nearest-neighbour distance). This effect is not as pronounced as before.\n-   There are more larger grains with increasing dilation-erosion, which means the convergence is clearer. However, there's little difference in the shape of the scatter plot between 10x10 px and 20x20 px.\n\nInterpretations:\n\n-   Increasing dilation-erosion means grains will generally grow in size, such that a lot of low-separation fine grains become merged, hence the increase in modal separation and spreading out towards larger grain sizes.\n\n"]},{"cell_type":"markdown","id":"07c1b6f9-9215-4f8e-ace4-6e57725b6cac","metadata":{},"source":["## Reflector Aspect Ratios\n\n"]},{"cell_type":"markdown","id":"dae1730c-a78d-4b71-a25e-c38eee5fba76","metadata":{},"source":["On the plots, the aspect ratio range (x-axis) is hardcoded (to between 0 and 20).\n\n"]},{"cell_type":"code","execution_count":1,"id":"735d102b-e5ab-4d07-aef4-d40986fb1f13","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["fig = plot_all(PLT.aspect_ratio_distros,file_prepend)\nfig.suptitle(\"Aspect Ratio Distributions\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-aspect-ratios.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"5be3e284-1414-4795-96e0-fa73280938a0","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-aspect-ratios.png\"></th>\n<th><img src=\"./imgs/modified-10-aspect-ratios.png\"></th>\n<th><img src=\"./imgs/modified-20-aspect-ratios.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"b791f63d-2494-4007-a5db-ef335f83cb13","metadata":{},"source":["### Discussion\n\n"]},{"cell_type":"markdown","id":"3dd3803d-87f8-42a0-ac6f-acdc0b20b7b4","metadata":{},"source":["Observations:\n\n-   The modal aspect ratio is nearest to 1.\n-   Aspect ratios are quite variable within each collection of samples with common degrees of alteration.\n-   Increasing dilation-erosion has little effect on the shape of these distributions.\n\nInterpretation:\n\n-   There's probably no confident information that can be extracted from these distributions due to a lack of consistency.\n-   However, the absence of significant effects on the shape of the distributions with increasing dilation-erosion provides further evidence for the robustness of this new method.\n\n"]},{"cell_type":"markdown","id":"826d1ac8-2ed6-4563-b303-67148aa09d1a","metadata":{},"source":["## Generalised Section Properties Processing\n\n"]},{"cell_type":"markdown","id":"5fcefd2f-8598-4743-abc7-e5384e2dd49c","metadata":{},"source":["The generalised section properties (table [1](#org3aadef7)) are section-specific (as opposed to grain-specific) properties that were initially though to be useful to compare between sections.\n\n\n| Property|Description|Units|\n|---|---|---|\n| <code>convhull</code>|area studied|mm<sup>2</sup>|\n| <code>n</code>|number of reflectors considered||\n| <code>total_area</code>|total area covered by reflectors|mm<sup>2</sup>|\n| <code>largest</code>|area of largest reflector|mm<sup>2</sup>|\n| <code>curve_fit</code>|area distribution fit parameters||\n| <code>alteration</code>|quantitative alteration degree||\n\n"]},{"cell_type":"code","execution_count":1,"id":"c7c9cc43-5654-403d-b3fe-a5e73ec754da","metadata":{},"outputs":[],"source":["# Check if the summaries datafile needs to be regenerated on the basis of missing file or request.\nif not os.path.exists(file_prepend + \"-summary.csv\") or FORCE_OVERWRITE:\n    data = dict()\n    # Iterate through samples and their area data.\n    for sample,sample_area_data in areas_data.items():\n        # Load patch areas.\n        patch_areas = sample_area_data[\"patch_areas\"]\n        # Load area studied.\n        area_studied = sample_area_data[\"area_studied\"]\n        # Load size of largest grain.\n        largest_grain = sample_area_data[\"largest_grain\"]\n        # Compute distribution parameters for patch areas.\n        # Note 99 rather than 100 as bin_values takes the number of bins rather than bin edges.\n        counts,_,midpoints = bin_values(patch_areas,0.05,99)\n\n        # Construct summary dataframe for each sample.\n        data[sample] = {\"convhull\":area_studied, # study area\n                        \"n\":len(patch_areas), # number of discrete reflectors after filtering\n                        \"total_area\":sum(patch_areas), # area of reflectors after filtering\n                        \"largest\":largest_grain, # largest continuous reflector patch area\n                        \"curve_fit\":fit_exp_log_y(midpoints,counts)}\n\n        # Degree of alteration assigned to each section.\n        # Note: alteration_degree is imported from plotting.py\n        try:\n            data[sample][\"alteration\"] = alteration_degree[sample]\n        except KeyError:\n            pass\n\n    # Convert dictionary to pandas dataframe.\n    df = pd.DataFrame.from_dict(data,orient=\"index\")\n    # Save pandas dataframe to .csv file.\n    df.to_csv(file_prepend + \"-summary.csv\")"]},{"cell_type":"markdown","id":"6d4514b7-9f2e-439f-9349-e6aad1b28a8c","metadata":{},"source":["### Comparison Plotting\n\n"]},{"cell_type":"markdown","id":"50079ab8-fe01-49eb-aa2a-8e3810995077","metadata":{},"source":["After obtaining this data, comparisons can be plotted.\n\n-   In some cases, derived parameters (that are normalised to the area studied) are more useful for comparing between sections.\n    -   Reflector coverage area &rarr; reflector coverage percentage.\n    -   Reflector count &rarr; reflector number density.\n-   Only sections that are partially (0) or heavily (1) altered will be considered in the comparison.\n\n"]},{"cell_type":"code","execution_count":1,"id":"906c60f2-40a6-4d12-9989-97d4cf8404f7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Force load from .csv file so that list processing is standardised.\ndf = pd.read_csv(file_prepend + \"-summary.csv\",index_col=0)\n# Derived parameters that are more logical to compare between sections.\ndf[\"reflector_percentage\"] = df[\"total_area\"]/df[\"convhull\"] * 100\ndf[\"number_density\"] = df[\"n\"]/df[\"convhull\"]\n\n# Look at only sections that have an alteration index of 1 (heavy) or 0 (partly).\ndf = df[(df[\"alteration\"]==1) | (df[\"alteration\"]==0)]\n\n######################################################\n# Comparison between aggregated reflector properties #\n######################################################\nfig,axs = plt.subplots(1,3,constrained_layout=True,figsize=(9,6))\n\n# Plot point for each sample's property.\naxs[0].scatter(df[\"alteration\"],df[\"largest\"])\naxs[1].scatter(df[\"alteration\"],df[\"number_density\"])\naxs[2].scatter(df[\"alteration\"],df[\"reflector_percentage\"])\n\n# Label the sample referred to by each point.\nfor s,row in df.iterrows():\n    x = row[\"alteration\"]\n    axs[0].text(x,row[\"largest\"],s)\n    axs[1].text(x,row[\"number_density\"],s)\n    axs[2].text(x,row[\"reflector_percentage\"],s)\n\n# Label the plots with which parameter is being compared.\naxs[0].set_ylabel(\"Largest reflector area /mm$^2$\")\naxs[1].set_ylabel(\"Reflector number density /mm$^-2$\")\naxs[2].set_ylabel(\"Reflector coverage /%\")\n\n# Label the plots with the degree of alteration represented by plotted samples.\n[ax.set_xlabel(\"Degree of alteration\") for ax in axs]\n[ax.set_xticks([0,1],[\"medium\",\"high\"]) for ax in axs]\n\nplt.suptitle(\"Reflector parameter comparisons between\\nmoderately and highly altered rocks\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-refl-param-comparison.png\"))\n\n#############################################\n# Comparison between area distribution fits #\n#############################################\nfig,axs = plt.subplots(1,2,constrained_layout=True,figsize=(6,6))\n\n# Load curve fit data.\ncurve_fits = np.array(json.loads(\"[\" + \",\".join(df[\"curve_fit\"]) + \"]\"))\n\n# Plot point for each sample's property.\naxs[0].scatter(df[\"alteration\"],curve_fits[:,0]/df[\"n\"])\naxs[1].scatter(df[\"alteration\"],curve_fits[:,1])\n\n# Label the plots with which parameter is being compared.\naxs[0].set_ylabel(\"a/n\")\naxs[1].set_ylabel(\"b\")\n\n# Label the sample referred to by each point.\nfor i,alt in enumerate(zip(curve_fits[:,0]/df[\"n\"],curve_fits[:,1])):\n    s = df.iloc[i].name\n    x = df.iloc[i][\"alteration\"]\n    axs[0].text(x,alt[0],s)\n    axs[1].text(x,alt[1],s)\n\n# Label the plots with the degree of alteration represented by plotted samples.\n[ax.set_xlabel(\"Degree of alteration\") for ax in axs]\n[ax.set_xticks([0,1],[\"medium\",\"high\"]) for ax in axs]\n\nplt.suptitle(\"Fit parameter values in area distribution curve fit of format: $10^{a \\cdot \\exp(b x)}$\")\nsave_figure(os.path.join(\"imgs\",file_prepend + \"-area_fit_param_comp.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"c811a410-728e-4859-9523-317a3953c867","metadata":{},"source":["For the area distribution curve fits, and interpretation of the parameters' meanings are:\n\n-   $a$: height of the distribution at the start such that $a/n$ is the height normalised by the number of reflectors (to permit comparison between sections). The larger $|a/n|$ is, the taller the start of the distribution relative to higher values.\n-   $b$: measure of \"decay\" rate of the negative exponential distribution. The larger $|b|$ is, the narrower the distribution.\n\n"]},{"cell_type":"markdown","id":"3cacf9fc-4951-4f23-b670-428454c62c3d","metadata":{},"source":["#### Reflector Parameter Comparison\n\n"]},{"cell_type":"markdown","id":"348c4eaf-da8f-49ea-b49b-dad9634cec04","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-refl-param-comparison.png\"></th>\n<th><img src=\"./imgs/modified-10-refl-param-comparison.png\"></th>\n<th><img src=\"./imgs/modified-20-refl-param-comparison.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"21eacbbf-0e92-4cba-9352-d56d1be4c887","metadata":{},"source":["##### Discussion\n\n"]},{"cell_type":"markdown","id":"fe7e4e4b-6b09-44fb-b79a-56b6a122b4ed","metadata":{},"source":["Observations:\n\n-   There's a narrowing of the range of values towards the smaller end for the largest parameter area with increasing alteration. This narrowing is more pronounced with increasing dilation-erosion.\n-   The reflector number density appears to broaden in range with increasing alteration.\n-   The reflector coverage appears to broaden in range and slightly increase with increasing alteration but only clearly so at 20x20 px dilation-erosion.\n\nInterpretation:\n\n-   Due to the greater effect of heterogeneity on larger grains, the difference in largest grain sizes can't be confidently interpreted.\n-   Broadening of number density and coverage suggests that increasing alteration can either have little effect on reflector number density, or can increase it.\n-   The effect of different amounts of dilation-erosion is not as important in determining how clear these changes in range are.\n\n"]},{"cell_type":"markdown","id":"c1e7ac93-d1ca-4595-bba4-4181682f23f4","metadata":{},"source":["#### Area Distribution Comparison\n\n"]},{"cell_type":"markdown","id":"a398b928-acf2-4a18-8cbe-a74348d7e8e7","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-area_fit_param_comp.png\"></th>\n<th><img src=\"./imgs/modified-10-area_fit_param_comp.png\"></th>\n<th><img src=\"./imgs/modified-20-area_fit_param_comp.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"3e21f223-94dc-4e9f-9cce-eadd941add1b","metadata":{},"source":["##### Discussion\n\n"]},{"cell_type":"markdown","id":"9191d7c3-0980-4b2e-aa66-5012d0cc5c19","metadata":{},"source":["Observations:\n\n-   $a/n$ generally broadens with increasing alteration. The amount of dilation-erosion has little effect on this.\n-   $b$ generally decreases lower magnitudes with increasing alteration, with this effect being more pronounced with increasing dilation-erosion.\n\nInterpretations:\n\n-   Increasing alteration can change the relative size of the lowest area bin in different directions.\n-   Increasing alteration generally broadens the area distribution (decreases magnitude of $b$), with this effect being more obvious with increasing dilation-erosion.\n\n"]},{"cell_type":"markdown","id":"0c67498e-042d-4160-be65-1e73f86818ee","metadata":{},"source":["## Sample Property Aggregation\n\n"]},{"cell_type":"markdown","id":"1f5294fe-151c-477b-969f-b32fb68fe086","metadata":{},"source":["Area distributions can be aggregated and differenced to make inferences on the grain population produced with increasing hydration.\n\nLooking at just the partially vs heavily altered sections (as the very heavily altered section just has one entry and is uncertain anyway):\n\n"]},{"cell_type":"code","execution_count":1,"id":"f278a85e-ed62-4b11-ab2d-8fe049c83585","metadata":{},"outputs":[],"source":["# Overwriting the imported sample list with just the samples of interest (i.e. that have alteration indices of either 0 or 1).\nalteration_degree = {k:v for k,v in alteration_degree.items() if v in [0,1]}"]},{"cell_type":"markdown","id":"8177f1ab-dbaa-438a-bd0d-42249de4bc46","metadata":{},"source":["Loading area data and defining how it's being binned:\n\n"]},{"cell_type":"code","execution_count":1,"id":"819de0bf-9ae0-4213-814d-615043bc1f4c","metadata":{},"outputs":[],"source":["with open(file_prepend + \"-areas.json\") as infile:\n    data = json.load(infile)\n\n# Hardcoded maximum area to define bins with.\nmax_area = 0.05 # mm^2\nbins = np.linspace(0,max_area,100)\n# Compute bin midpoints.\nmidpoints = (bins[1:] + bins[:-1])/2\n# Function to normalise data.\nnorm = lambda x : np.array(x)/sum(x)"]},{"cell_type":"markdown","id":"df804357-bb1b-44fc-8cbc-b812fe703c73","metadata":{},"source":["Grouping normalised area distributions by degree of alteration, with each distribution weighted by how much area was studied to produce the distribution.\n\n"]},{"cell_type":"code","execution_count":1,"id":"e81eb8c2-0ebf-421c-98d0-6b541a3eae44","metadata":{},"outputs":[],"source":["# Declare dictionary in which data will be aggregated.\ngrouped_data = dict()\n# Iterate through sample data.\nfor key,area_data in data.items():\n    # Extract areas data.\n    areas = area_data[\"patch_areas\"]\n    # Extract the area studied.\n    studied_area = area_data[\"area_studied\"]\n    # Check if the sample is of interest.\n    if key in alteration_degree:\n        # If so, extract the degree of alteration of the sample.\n        alteration = alteration_degree[key]\n        # Check if the degree of alteration of interest already has a preallocated data structure in the top-level dictionary dataframe.\n        if not alteration in grouped_data:\n            # If not, create this data structure.\n            grouped_data[alteration] = {\"distribution\":[],\n                                        \"n\":0}\n        # Compute area distribution via histogram.\n        counts,_ = np.histogram(areas,bins=bins)\n        # Normalise the distribution.\n        normed_counts = norm(counts)\n        # Weight the distribution by the amount of area studied to produce that distribution.\n        weighted_counts = studied_area * normed_counts\n        # Store the distribution.\n        grouped_data[alteration][\"distribution\"].append(weighted_counts)\n        # Add to the number of reflector patches considered for sections of the active degree of alteration.\n        grouped_data[alteration][\"n\"] += len(areas)\n\n# Aggregate and normalise the distributions.\npartially_altered = norm(np.sum(np.array(grouped_data[0][\"distribution\"]),axis=0))\nheavily_altered = norm(np.sum(np.array(grouped_data[1][\"distribution\"]),axis=0))"]},{"cell_type":"markdown","id":"c452633a-a24e-4987-a595-a70485481601","metadata":{},"source":["Fitting a combined exponential and order 1 polynomial decay function to the distributions, and then saving the results of the fit to permit later investigation of the robustness of difference of distributions.\n\n-   Note: the combined exponential-polynomial function is used here since fitting with just an exponential function (as with the individual area distributions) produces nonsensical results - this can be seen when `fit_func` is changed to `exp_func`.\n\n"]},{"cell_type":"code","execution_count":1,"id":"a1a46a24-fccf-4c6f-a180-a840f093a14b","metadata":{},"outputs":[],"source":["# Function used for fitting.\nfit_func = exp_with_first_order_p_func\n\n# Only fit to positive values (i.e. where the count is not zero).\nfitting_p = partially_altered>0\nfitting_h = heavily_altered>0\n\n# Determine fit parameters.\npopt_p,_ = curve_fit(fit_func,\n                     midpoints[fitting_p],np.log10(partially_altered[fitting_p]))\npopt_h,_ = curve_fit(fit_func,\n                     midpoints[fitting_h],np.log10(heavily_altered[fitting_h]))\n\n# Save fit parameters.\nwith open(file_prepend + \"-distribution_fits.json\",\"w\") as outfile:\n    json.dump({\"partial\":popt_p.tolist(),\n               \"heavy\":popt_h.tolist(),\n               \"bins\":bins.tolist()},\n              outfile)"]},{"cell_type":"markdown","id":"806dac4f-b0a8-4806-b5ad-4b3d017c84c3","metadata":{},"source":["### Plotting Aggregated Distributions\n\n"]},{"cell_type":"code","execution_count":1,"id":"bd57bb03-a8a4-4951-8439-9c999c319b00","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Plot the aggregated area distribution for partially altered samples, as well as the fit.\nplt.stairs(partially_altered,bins,label=\"partially\",color=\"b\")\nplt.plot(midpoints,10**fit_func(midpoints,*popt_p),c=\"b\")\n# Plot the aggregated area distribution for heavily altered samples, as well as the fit.\nplt.stairs(heavily_altered,bins,label=\"heavily\",color=\"g\")\nplt.plot(midpoints,10**fit_func(midpoints,*popt_h),c=\"g\")\n\n# Set y axis to log scale.\nplt.gca().set_yscale(\"log\")\n# Label axes.\nplt.xlabel(\"Area /mm$^2$\")\nplt.ylabel(\"Frequency\")\n# Display legend.\nplt.legend()\n\nsave_figure(os.path.join(\"imgs\",file_prepend+\"-partially-vs-heavily-altered.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"1ff5941e-5c3d-4fe3-8e22-f164e103a5a0","metadata":{},"source":["Generally speaking, these fits are not great &#x2026;\n\n<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-partially-vs-heavily-altered.png\"></th>\n<th><img src=\"./imgs/modified-10-partially-vs-heavily-altered.png\"></th>\n<th><img src=\"./imgs/modified-20-partially-vs-heavily-altered.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"04ad91df-3ad9-4049-97e9-91cb321d6421","metadata":{},"source":["#### Discussion\n\n"]},{"cell_type":"markdown","id":"84314f1f-82be-45ab-b514-32c8d353ce27","metadata":{},"source":["<span class=\"timestamp-wrapper\"><span class=\"timestamp\">&lt;mer. d√©c.  6 2023 15:48&gt;</span></span>\nObservations:\n\n-   These fits aren't great (even ignoring the semilog nature of these plots)\n\nInterpretation:\n\n-   A better fit function may be needed - or manually drawing continuous distributions?\n\n"]},{"cell_type":"markdown","id":"4b9a0b28-9b15-478e-a452-4f047b57adf5","metadata":{},"source":["### Plotting Differenced Distributions\n\n"]},{"cell_type":"markdown","id":"177d8cea-00aa-427d-836a-0b1ca67d6c46","metadata":{},"source":["Plotting the difference in heavily altered distribution and partially altered distribution to characterise the change following increasing alteration.\n\n"]},{"cell_type":"code","execution_count":1,"id":"82c846a8-82e3-4451-862e-323ac8158af9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Compute difference in distributions.\ndiff = heavily_altered-partially_altered\n# Plot horizontal line at y=0.\nplt.axhline(0,c=\"lightblue\",linestyle=\"--\")\n# Plot difference in distributions.\nplt.stairs(diff,bins,label=\"heavily-partially altered freqs.\",color=\"k\")\n# Label axes.\nplt.xlabel(\"Area /mm$^2$\")\nplt.ylabel(\"Heavily minus Partially altered Freq. Diff.\")\n\nsave_figure(os.path.join(\"imgs\",file_prepend+\"-heavily-minus-partially-altered.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"4224c8d2-a817-4fa2-b520-3307b64d35ef","metadata":{},"source":["<table>\n<tr>\n<th style=\"text-align:center\">No dilation-erosion</th>\n<th style=\"text-align:center\">10x10 px kernel dilation-erosion</th>\n<th style=\"text-align:center\">20x20 px kernel dilation-erosion</th>\n</tr>\n<tr>\n<th><img src=\"./imgs/unmodified-heavily-minus-partially-altered.png\"></th>\n<th><img src=\"./imgs/modified-10-heavily-minus-partially-altered.png\"></th>\n<th><img src=\"./imgs/modified-20-heavily-minus-partially-altered.png\"></th>\n</tr>\n</table>\n\n"]},{"cell_type":"markdown","id":"729557a6-34fd-4e40-9ede-2c19621c0ec3","metadata":{},"source":["#### Discussion\n\n"]},{"cell_type":"markdown","id":"02f595b2-2fcf-4fd0-a637-105d6199562e","metadata":{},"source":["The previous observation that there's a decrease in the proportion of some finer grain size, with an increase in grains just coarser, and that increase decaying with increasing grain size up to ~0.02 mm<sup>2</sup> (as opposed to 0.01 mm<sup>2</sup> before) is still present but much weaker.\n\n-   The previous interpretation that this was caused by a process that removed finer grains and \"reprecipitated\" them on other fine grains to coarsen then is still correct, but the mechanism is not geological, but rather an artefact of the dilation-erosion process: smaller grains are merged by this processing (removing the number of smaller grains but increasing the amount of slightly larger grains). The effect of this is especially clear when a 20x20 px kernel is used.\n\nSo reduce the effect of randomness, a smaller number of bins can be used when plotting the diagram of focus (i.e. results of processing with the 10x10 px kernel).\n\n![img](./imgs/modified-10-heavily-minus-partially-altered-bins50.png \"50 bins\")\n\n![img](./imgs/modified-10-heavily-minus-partially-altered-bins20.png \"20 bins\")\n\nThough there does seem to be a very approximate decrease in the proportion of finer grains, followed by an increase in the proportion of slightly coarser grains, this effect is not as clear.\n\nThe potentially oscillatory nature of the difference in distributions that \"dampens\" with increasing reflector area may also suggest this is an effect of random variation whose magnitude decreases with lesser observations (i.e. there were many small grains observed so randomness could produce significant variations in the normalised aggregated distributions around the small-area end of the distribution; there were few larger grains observed so even if there was randomness, the smaller numbers within the tail of the area distribution means it would appear smaller on the plot of difference).\n\nOne possible way of testing this hypothesis is by dividing the each bin in the difference graph by the inverse of the sum of the frequencies in the corresponding bin from the area distribution (i.e. such that the resultant metric is effectively a measure of how large the change is relative to how many samples are in the bin). If randomness was in fact the cause, then the resultant \"normalised\" difference should oscillate with increasing magnitude at larger areas (since smaller counts should result in a greater effect of randomness).\n\n"]},{"cell_type":"code","execution_count":1,"id":"faaa09e1-e288-457c-97d5-009af661c1b9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Compute difference in distributions.\ndiff = heavily_altered-partially_altered\nbin_sum = heavily_altered+partially_altered\nnormed_diff = diff/bin_sum\n# Plot horizontal line at y=0.\nplt.axhline(0,c=\"lightblue\",linestyle=\"--\")\n# Plot difference in distributions\nplt.stairs(normed_diff,bins,label=\"heavily-partially altered freqs.\",color=\"k\")\nplt.xlabel(\"Area /mm$^2$\")\nplt.ylabel(\"\\\"Normalised\\\" Heavily minus Partially altered Freq. Diff.\")\nsave_figure(os.path.join(\"imgs\",file_prepend+\"-heavily-minus-partially-altered-normed.png\"))\nplt.show()"]},{"cell_type":"markdown","id":"e4b656d3-02c6-4a6a-b256-971041fcfc1a","metadata":{},"source":["This **is** seen, which means the difference at very small areas is insufficiently robust to interpret further (i.e. is more likely an inherent effect of the data rather than geological processes).\n\nHowever, the gradient of increase is steeper than the gradient of decrease, which provides further evidence for there broadly being more coarser grains in the heavily altered vs partly altered sections even at small areas in the area distribution (i.e. the area distributions for the heavily altered sections are generally broader than the area distributions for the partly altered sections).\n\n-   Where the normalised difference is -1 or 1, it means that in one of the distributions, there's zero frequency within that bin, but in the other distribution, there's a non-zero frequency.\n\n![img](./imgs/annot-modified-10-heavily-minus-partially-altered-normed.png)\n\nSince the effect of \"normalisation\" means the difference is amplified with reducing number of observations, and the number of observations reduces with size, normalisation effectively amplifies the effect of differences in larger grains.\n\n"]},{"cell_type":"code","execution_count":1,"id":"63ebf0f5-bf1b-4cf8-8b09-1a6f5c19150c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["positive_normed_diff = sum(normed_diff[normed_diff > 0])\nnegative_normed_diff = abs(sum(normed_diff[normed_diff < 0]))\nprint(f\"Summed positive change {positive_normed_diff}\\nSummed negative change {negative_normed_diff}\")"]},{"cell_type":"markdown","id":"c09346b1-f121-4d9f-8e5c-74d103f59375","metadata":{},"source":["Since there's more net positive change than negative change, it can be concluded that increasing alteration increases frequencies in the tail (larger areas) of the area distribution - which contradicts the curve fits above (where the tail of the fit to heavily altered distribution is below that of the partly altered distribution).\n\n"]},{"cell_type":"markdown","id":"7981f896-5534-4aeb-b29d-f2b1e93ce360","metadata":{},"source":["### Random Sampling Analysis\n\n"]},{"cell_type":"markdown","id":"a2bfcb73-85e0-46f7-9bed-43e34dd326ab","metadata":{},"source":["To confirm some of the inferences made in the previous discussion on the shape of the difference, random sampling can be used:\n\n1.  Generate two random samples from the same distribution (e.g. partially altered grain areas distribution). The size of these samples should correspond to the sizes of the samples used to produce the aggregated distributions (i.e. one sample should have the same size as the data used to generate the aggregated partially-altered distribution, and the other the same size as the data used to generate the aggregated heavily-altered distribution). This will result in uneven sample sizes.\n2.  Find the normalised distribution of these random samples\n3.  Find the difference of these normalised distributions in the same direction as used with the real data (i.e. the random sample that is the same size as the heavily-altered distribution minus the random sample that is the same size as the less heavily-altered distribution). Find also the relative difference in the same was as with the real data (difference of bins/sum of bins).\n4.  Plot the two types of difference.\n\nSince the two random samples are coming from the same distribution, if the resulting pattern is similar to the ones that were interpreted in the context of different distributions above, then those interpretations are invalidated since it would suggest those patterns could be a result of random sampling from the <u>same</u> distribution.\n\n-   Note: though a curve fit is used as the probability distribution for generating random samples, the fact that only one curve fit is used means the issue of unsuitability for use in comparing different distributions is avoided.\n\n"]},{"cell_type":"code","execution_count":1,"id":"0d8d4da5-1962-45f1-88c0-a3b27d1fa7d8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Number of items in each sample based on how many were present in the data.\nn_partly_altered = grouped_data[0][\"n\"]\nn_heavily_altered = grouped_data[1][\"n\"]\n\n# Print number of items in each sample.\nprint(f\"Number in first sample (partly-altered): {n_partly_altered}\\nNumber in second sample (heavily-altered): {n_heavily_altered}\")\n\n# Load the distribution fits data.\nwith open(file_prepend + \"-distribution_fits.json\") as infile:\n    data = json.load(infile)\n# Extract fit parameters.\nfit_p = data[\"partial\"]\nfit_h = data[\"heavy\"]\n# Extract bins used to produce the distributions that the fit parameters were derived from.\nbins = np.array(data[\"bins\"])\n# Compute the midpoints of these bins.\nmidpoints = (bins[:-1] + bins[1:])/2\n\n# Function to construct a discrete probability distribution (for specified x values) applicable to the fit function used to produce the fit parameters above.\np_x = lambda x,fit : 10**exp_with_first_order_p_func(x,*fit)\n# Various partial functions:\np_x_p = lambda x : p_x(x,fit1)\np_x_h = lambda x : p_x(x,fit2)\np = lambda fit : p_x(midpoints,fit)\n# Discrete probability distributions.\np_p = p(fit_p)\np_h = p(fit_h)\n\n# Function to normalise data.\nnorm = lambda x : x/sum(x)\n\n# Initiate random number generator.\nrng = np.random.default_rng()\n\n# Random sample based on the partially-altered distribution with the first sample's size.\nrand_a = rng.choice(midpoints,size=n_partly_altered,p=norm(p_p))\n# Random sample based on the heavily-altered distribution with the second sample's size.\nrand_b = rng.choice(midpoints,size=n_heavily_altered,p=norm(p_p))\n\n# Bin the data.\ncount_a,_ = np.histogram(rand_a,bins)\ncount_b,_ = np.histogram(rand_b,bins)\n# Normalise the binned data (i.e. to produce frequencies from counts).\ncount_a = norm(count_a)\ncount_b = norm(count_b)\n# Find the absolute difference in normalised data.\ndiff = count_b-count_a\n# Find the relative difference in normalised data.\ndiff_relative = diff/(count_b+count_a)\n# Find the net additions (sum of positive bins of relative difference).\nadditions = sum(diff_relative[diff_relative>0])\n# Find the net removals (magnitude of the sum of negative bins of relative difference).\nremovals = abs(sum(diff_relative[diff_relative<0]))\n\n# Define a plot layout.\nfig,axs = plt.subplots(2,1,constrained_layout=True,figsize=(6,6))\n\n# Plot horizontal line at y=0.\naxs[0].axhline(0,c=\"lightblue\",linestyle=\"--\")\n# Plot the \"distribution\" of absolute difference in normalised data.\naxs[0].stairs(diff,bins)\naxs[0].set_ylabel(\"Difference in frequency\")\n\n# Plot horizontal line at y=0.\naxs[1].axhline(0,c=\"lightblue\",linestyle=\"--\")\n# Plot the \"distribution\" of relative difference in normalised data.\naxs[1].stairs(diff_relative,bins)\naxs[1].set_ylabel(\"Relative difference in frequency\")\n\n# Label just the x axis of the lower plot (since the horizontal axes are the same).\naxs[1].set_xlabel(\"Area /mm$^2$\")\n# Display the net addition and net removals (relevant to the relative differences) in the title.\naxs[1].set_title(f\"Net addition: {additions:.2f}\\nNet removal: {removals:.2f}\")\n\nplt.show()"]},{"cell_type":"markdown","id":"75894e12-09bb-4cdf-b9cf-9d8177b0d818","metadata":{},"source":["In the plot of absolute difference in frequency against grain area, it's clear that there is in fact an oscillatory trend that decreases with increasing area, which means the observed trend of a decreased proportion of finest grains with increased proportions of slightly coarser grains is not robust and so should not be interpreted.\n\nIn the plot of relative difference in frequency against grain area, there's usually a greater net addition (sum of positive bins) than net removal (sum of negative bins). Though the shapes of these distributions can't be strengthened by repeats (repeats would reduce differences since they're related to randomness - i.e. the trends would approach a horizontal line at zero), repeats could be used to investigate the difference between net addition and removal.\n\n"]},{"cell_type":"code","execution_count":1,"id":"57262a21-5269-41d8-816c-2df501a59e43","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Number of repeats.\nn_repeats = 500\n\n# Define lists to which the observed net additions and removals (in relative differences) can be stored.\nall_additions = []\nall_removals = []\n\n# Repeat the \"experiment\".\nfor i in range(n_repeats):\n    # Random sample based on the partly-altered distribution with the first sample's size.\n    rand_a = rng.choice(midpoints,size=n_partly_altered,p=norm(p_p))\n    # Random sample based on the heavily-altered distribution with the second sample's size.\n    rand_b = rng.choice(midpoints,size=n_heavily_altered,p=norm(p_p))\n\n    # Bin the data.\n    count_a,_ = np.histogram(rand_a,bins)\n    count_b,_ = np.histogram(rand_b,bins)\n    # Normalise the binned data (i.e. to produce frequencies from counts).\n    count_a = norm(count_a)\n    count_b = norm(count_b)\n    # Find the absolute difference in normalised data.\n    diff = count_b-count_a\n    # Find the relative difference in normalised data.\n    diff_normed = diff/(count_b+count_a)\n    # Find the net additions (sum of positive bins of relative difference).\n    additions = sum(diff_normed[diff_normed>0])\n    # Find the net removals (magnitude of the sum of negative bins of relative difference).\n    removals = abs(sum(diff_normed[diff_normed<0]))\n    # Store the observed net additions.\n    all_additions.append(additions)\n    # Store the observed net removals.\n    all_removals.append(removals)\n\n# Produce boxplots showing the distribution of net additions and net removals.\nplt.boxplot([all_additions,all_removals])\n# Label axes.\nplt.ylabel(\"Net/Sum\")\nplt.gca().set_xticks([1,2],[\"additions\",\"removals\"])\n\nplt.title(\"Difference between net additions and removals in relative differences\")\nplt.show()"]},{"cell_type":"markdown","id":"2d507647-f1de-48fd-b0b8-3cac4b27613d","metadata":{},"source":["From this plot, it's clear that randomness alone <u>can</u> generate larger net addition than net removal. Hence the previous interpretation that the larger net addition represented an increase in the tail of the area distribution is not robust. Instead, it's likely this difference is caused by a difference in sample sizes (since that's the only other \"variable\"). Furthermore previous interpretations on the initial gradients (magnitude of positive vs negative gradient) will also be discounted.\n\nTo verify whether the difference between net addition and net removal is a result of uneven sample sizes, the above analysis is repeated below with the same sample sizes (above code block repeated but with the random samples being the same size):\n\n"]},{"cell_type":"code","execution_count":1,"id":"edd62dee-ec63-48ca-a33a-42dd03f9c6f6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Number of repeats.\nn_repeats = 500\n\n# Define lists to which the observed net additions and removals (in relative differences) can be stored.\nall_additions = []\nall_removals = []\n\n# Repeat the \"experiment\".\nfor i in range(n_repeats):\n    # Random sample based on the partly-altered distribution with the first sample's size.\n    rand_a = rng.choice(midpoints,size=n_partly_altered,p=norm(p_p))\n    # Random sample based on the heavily-altered distribution with the second sample's size.\n    rand_b = rng.choice(midpoints,size=n_partly_altered,p=norm(p_p))\n\n    # Bin the data.\n    count_a,_ = np.histogram(rand_a,bins)\n    count_b,_ = np.histogram(rand_b,bins)\n    # Normalise the binned data (i.e. to produce frequencies from counts).\n    count_a = norm(count_a)\n    count_b = norm(count_b)\n    # Find the absolute difference in normalised data.\n    diff = count_b-count_a\n    # Find the relative difference in normalised data.\n    diff_normed = diff/(count_b+count_a)\n    # Find the net additions (sum of positive bins of relative difference).\n    additions = sum(diff_normed[diff_normed>0])\n    # Find the net removals (magnitude of the sum of negative bins of relative difference).\n    removals = abs(sum(diff_normed[diff_normed<0]))\n    # Store the observed net additions.\n    all_additions.append(additions)\n    # Store the observed net removals.\n    all_removals.append(removals)\n\n# Produce boxplots showing the distribution of net additions and net removals.\nplt.boxplot([all_additions,all_removals])\n# Label axes.\nplt.ylabel(\"Net/Sum\")\nplt.gca().set_xticks([1,2],[\"additions\",\"removals\"])\n\nplt.title(\"Difference between net additions and removals in relative differences\")\nplt.show()"]},{"cell_type":"markdown","id":"3d6d4b61-75eb-4907-b1a8-584f7420b4e9","metadata":{},"source":["The similar net additions and removals when using the same sample size confirms the suspicion that differences in sample size was the cause.\n\nWith sample size affecting area distribution so much, another issue with the above analysis arises - are the partly-altered sample distributions thinner than the heavily-altered sample distributions due to a geological or statistical mechanism? Could thinner distributions be generated by subsampling a thicker distribution?\n\n"]},{"cell_type":"markdown","id":"4100a0c1-a46f-47d9-a690-b8eac5abc3e8","metadata":{},"source":["## Are Partially Altered Area Distributions Thinner due to Geological or Statistical Mechanisms?\n\n"]},{"cell_type":"markdown","id":"248336a5-6f3a-4726-9823-ec982509280c","metadata":{},"source":["Looking at non-aggregated area distribution shapes:\n\n1.  Take a continuous distribution of the form $n = 10^{\\alpha\\exp(\\beta A)}$ where $n$ is count, $A$ is area and $\\alpha$ and $\\beta$ are constants (fit parameters). The constants of this parent distribution are set to realistic values as determined by the individual-section area distribution analyses.\n2.  Discretise this continuous distribution. Let the sum of the bins of this continuous distribution be $N$.\n3.  Subsample at a range of realistic sample sizes.\n4.  \"Normalise\" the subsamples to $N$ (i.e. such that the subsamples have the same number of items as $N$ but with the same shape). Do this by dividing the subsamples' area distribution by the relevant factor.\n5.  Fit curves of the same form as mentioned in step 1 to distributions represented by these subsamples and recover the fit parameters.\n6.  Repeat many times to get distribution of $\\beta$ values at the range of sample sizes.\n7.  Repeat this with a range of reasonable $\\beta$ values for step 1.\n\n"]},{"cell_type":"code","execution_count":1,"id":"11c831ba-b54e-460c-9fda-baf08ab40b57","metadata":{},"outputs":[],"source":["# Number of repeats.\nn_repeats = 500\n\n# One of the parent distribution's \"fit\" parameter.\nalpha = 3 # Approximate\n\n# Name of the popt parameters for the fit function (hardcoded to be exp_func).\npopt_params = [\"alpha\",\"beta\"]\n# Which popt parameter to be plotted (0: alpha; 1: beta).\n# If this is changed to 0, then the range of values to be tested (parent_betas) will need to change too.\npopt_idx = 1\n\ndef repeat_fit(random_sample_size,choices,p,n_repeats,rng=None):\n    ''' Compute fit parameters (for the specific exponential function) of the distribution of random samples of a parent distribution. Repeat this computation the specified number of times to get a collection of fit parameters.\n\n    random_sample_size (int) : size of the random sample.\n    choices (list of numericals) : numbers to chose from.\n    p (list of numericals) : probability weighting corresponding to the choices. Note: this function is tailored for the analysis in that p should be a discretisation of the parent distribution, with the sum of p being the number of reflector grains represented by this parent distribution.\n    n_repeats (int) : number of repeats.\n    rng : type of rng to use. Defaults to numpy's default rng if not provided.\n\n    returns popts : list of fit parameters with length equal to the number of repeats.\n    '''\n    # Allocate a default RNG \"method\" if None provided.\n    if rng is None:\n        rng = np.random.default_rng()\n    # Compute number of items represented by p (weightings).\n    n_p = sum(p)\n    # Declare list in which fit parameter can be stored.\n    popts = []\n    # Repeat the specified number of times.\n    for i in range(n_repeats):\n        # Get a random sample of specified size from specified choices with specified weightings.\n        rand = rng.choice(choices,size=random_sample_size,p=norm(np.array(p)))\n        # Bin the random sample.\n        count,_ = np.histogram(rand,len(choices))\n        nonzero = count!=0\n        # Determine the normalisation factor required to match the size of the random sample to the size of the discretised parent distribution to ensure comparability in fit parameters.\n        norm_factor = random_sample_size/n_p\n        # Normalise binned random sample in the way described above.\n        count = count[nonzero]/norm_factor\n        # Determine fit parameters to exp_func after applying log10 to the normalised binned data counts.\n        popt,_ = curve_fit(exp_func,midpoints[nonzero],np.log10(count))\n        popts.append(list(popt))\n    # Convert the list of fit parameters into a numpy array.\n    popts = np.array(popts)\n    return popts\n\n# This is a computationally-expensive plot, so will only be reproduced if the figure isn't already present in the ./imgs folder or FORCE_OVERWRITE is True.\nbootstrap_variability_img = os.path.join(\".\",\"imgs\",f\"{file_prepend}-{popt_params[popt_idx]}-variability.png\")\nif not os.path.exists(bootstrap_variability_img) or FORCE_OVERWRITE:\n    # Sample sizes to iterate through.\n    sample_sizes = 10**np.linspace(3,4.5,8)\n    # Parent beta values to iterate through.\n    parent_betas = [-100,-300,-1000]\n    # Iterate through the parent beta values.\n    for i,beta in enumerate(parent_betas):\n        # Define the continuous area distribution function.\n        continuous_distribution = lambda x : 10**exp_func(x,alpha,beta)\n        # Discretise the continuous area distribution at area values (midpoints) that have remained consistent throughout this notebook (i.e. range between 0 to 0.05 mm^2).\n        p = continuous_distribution(midpoints)\n        # Reduce the number of inputs into the function to find parameter fits by assigning values that don't change at least within each iteration to them.\n        standard_repeat_fit = lambda random_sample_size : repeat_fit(random_sample_size,midpoints,p,n_repeats,rng)\n        # Get a collection of fit parameters for the range of sample sizes specified above.\n        popts = np.array([standard_repeat_fit(int(n)) for n in sample_sizes])\n        # Extract the beta values from these fit parameters.\n        betas = popts[:,:,popt_idx]\n\n        # Verify that curve fitting works by fitting the discretised propability distribution.\n        real_fit_popt,_ = curve_fit(exp_func,midpoints,np.log10(p))\n        if real_fit_popt[popt_idx] == beta:\n            print(\"Verified\")\n        else:\n            print(\"Verification failed\")\n\n        # Label only one parent beta line.\n        if i==0:\n            label = \"Actual/Input $\\\\%s$\" % popt_params[popt_idx]\n        else:\n            label = None\n        # Plot the parent beta line for this iteration.\n        plt.axhline(real_fit_popt[popt_idx],label=label,color=\"lightblue\",linestyle=\"--\")\n        # Plot the distributions of observed betas at different sample sizes for this iteration (for a constant parent beta).\n        plt.boxplot(list(betas),positions=sample_sizes,widths=sample_sizes/5)\n\n    # Label axes.\n    plt.ylabel(\"$\\\\%s$\" % popt_params[popt_idx])\n    plt.xlabel(\"Sample size\")\n    # Show legend.\n    plt.legend()\n    # Set x axis scale to log.\n    plt.gca().set_xscale(\"log\")\n\n    plt.savefig(bootstrap_variability_img)\n    plt.show()"]},{"cell_type":"markdown","id":"246dd357-2d53-4446-9620-ff2f1450b911","metadata":{},"source":["![img](./imgs/beta-variability.png \"Results\")\n\nThis diagram not only shows that for random sample sizes on the order of 1000s, the observed &beta; is not the same as the parent distribution's &beta;, but also that the direction of difference is dependent on both the sample size and the parent &beta;. The larger the sample size, or the less negative the value of parent &beta;, the closer the observed &beta; values are to the parent &beta; (i.e. there's convergence of observed &beta; values onto the parent &beta; value).\n\n-   Furthermore, the amount of uncertainty (proxied by the range) decreases with increasing sample size or more negative parent &beta;.\n-   These uncertainties are significant relative to the magnitude of the parent &beta; (on the order of ~1/4 at sample sizes of 1000).\n\nThe combined effect of all this complexity and non-linearity means that it's unsuitable to interpret differences in observed &beta; values in the data. Hence, the suggestion that increasing alteration broadens the distributions is not robust.\n\n"]},{"cell_type":"markdown","id":"a5fae5a0-9fb4-4c87-96c1-54f2e345bb6e","metadata":{},"source":["## Using Bootstrapping Methods to Estimate Uncertainty [12 Dec 2023]\n\n"]},{"cell_type":"markdown","id":"596299bb-9161-4fed-804e-921c82a2dae8","metadata":{},"source":["The above analysis reveals that it's possible to estimate the parent &beta; from the observed &beta; (derived from data) assuming the observed &beta; is close to the median of &beta; values derived from random samples. The distribution of &beta; values derived from these random samples can be used as an *approximate* proxy for the range of parent &beta; values that can generate observed distributions.\n\n-   Note: &alpha; values are assumed to be correct in this analysis - in reality these have uncertainties too. However a focus on comparing &beta; values between the samples means &beta; values are focused on more.\n\nThe process for finding a suitable parent &beta; given sample parameters involves an iterative process that minimises the difference between the empirical median &beta; (derived from subsampling some parent &beta;) and observed &beta;, such that the \"best fit\" parent &beta; is one that could \"best\" generate the observed sample.\n\n-   However, due to the random sampling involved, the \"best fit\" parent &beta; is non-unique.\n-   Therefore, this analysis is approximate and more so intended to give a sense of whether differences in observed &beta; values are robust or not.\n\nSince this analysis computation expensive, it will not be repeated by default (change `FORCE_OVERWRITE` or delete the output `*-betas.json` to force repeat, which will also overwrite the saved figures).\n\n"]},{"cell_type":"code","execution_count":1,"id":"e34aa1c3-0c8f-47f3-a47e-e86b7a16f723","metadata":{},"outputs":[],"source":["def sample_empirical_betas(parent_beta,alpha,n,n_repeats=1000):\n    ''' Return a list of beta values produced from fits to subsamples of a discretised continuous distribution specific to this analysis. Variations in alpha values are ignored. Also verifies if the parent_beta can be recovered by fitting to the discretised distribution at integer resolution.\n\n    parent_beta (numerical) : beta value for the continuous distribution\n    alpha (numerical) : alpha value for the continuous distribution\n    n (int) : number of items in each sumsample\n    n_repeats (int) : number of repeats = number of elements in the list of returned beta values\n\n    returns betas (list of numericals) : list of \"observed\" beta values\n    '''\n    popt_idx = 1\n    # Define the continuous area distribution function.\n    continuous_distribution = lambda x : 10**exp_func(x,alpha,parent_beta)\n    # Discretise the continuous area distribution at area values (midpoints) that have remained consistent throughout this notebook (i.e. range between 0 to 0.05 mm^2).\n    p = continuous_distribution(midpoints)\n    # Reduce the number of inputs into the function to find parameter fits by assigning values that don't change at least within each iteration to them.\n    standard_repeat_fit = lambda random_sample_size : repeat_fit(random_sample_size,midpoints,p,n_repeats,rng)\n    # Get a collection of fit parameters for sample size specified.\n    popts = standard_repeat_fit(n)\n    # Extract the beta values from these fit parameters.\n    betas = popts[:,popt_idx]\n\n    # Verify that curve fitting works by fitting the discretised propability distribution.\n    real_fit_popt,_ = curve_fit(exp_func,midpoints,np.log10(p))\n    if int(real_fit_popt[popt_idx]) == int(parent_beta):\n        print(\"Verified\")\n    else:\n        print(\"Verification failed\")\n    return betas\n\ndef parent_beta_search(observed_beta,alpha,n,similarity_tolerance=0.5,n_repeats=1000,max_iter=50):\n    ''' Find a parent beta that will produce a median similar to the observed beta. Note: due to the random sampling required to determine a median, the solution is non-unique. However, plotting the outputted parent_beta_arr against empirical_beta_arr can reveal the approximate relation between the parent beta and empirical median beta after subsampling it (as well as the non-unique nature of this relation).\n\n    observed_beta (numerical) : observed beta derived from the real data\n    alpha (numerical) : observed alpha derived from the real data\n    n (int) : number of items within the real data, which informs the number of items per subsample\n    similarity_tolerance (numerical) : if the absolute of the difference between empirical median beta and observed beta is below this, then treat them as the same and accept the parent beta used to generate that median\n    n_repeats (int) : number of subsampling experiment repeats before finding a median\n    max_iter (int) : maximum number of tries in finding a suitable parent beta that produces match between empirical median beta and observed beta. If this is exceeded, the arrays will still be returned (for closest-match finding), but betas will be None\n\n    returns parent_beta_arr (list of numericals) : list of parent beta values (values used for generating the discretised continuous distribution)\n    empirical_beta_arr (list of numericals) : list of empirical median beta values (determined from a set of beta values obtained from subsamples)\n    betas (list of numericals) : list of empirical beta values obtained from subsample in the final iteration (i.e. after a suitable parent beeta is found)\n    '''\n    print(\"Target: reducing absolute difference below %.3f\" % similarity_tolerance)\n    parent_beta = observed_beta\n    # Produce first subsample of betas.\n    betas = sample_empirical_betas(parent_beta,alpha,n,n_repeats)\n    # Find median of subsample of betas.\n    median = np.median(betas)\n    # Find difference between empirical median from random sample and the beta value derived from actual data.\n    beta_diff = observed_beta - median\n    # Define list in which parent beta values are stored.\n    parent_beta_arr = [parent_beta]\n    # Define list in which resultant empirical median beta values are stored.\n    empirical_beta_arr = [median]\n    # Variable that stores the current iteration.\n    i = 0\n    # Iteratively \"improve\" the parent beta value as long as there's no match and the current iteration is below maximum.\n    while abs(beta_diff) > similarity_tolerance and i < max_iter:\n        print(\"Parent beta: %.3f, Empirical beta: %.3f;\\nAbs. Difference: %.3f\" % (parent_beta,median,beta_diff))\n        # Counteract the direction of difference between parent beta and empirical median beta (based on the signed nature of beta_diff).\n        parent_beta = parent_beta + beta_diff\n        # Produce first subsample of betas.\n        betas = sample_empirical_betas(parent_beta,alpha,n)\n        # Find median of subsample of betas.\n        median = np.median(betas)\n        # Find difference between empirical median from random sample and the beta value derived from actual data.\n        beta_diff = observed_beta - median\n        # Store this iteration's parent beta.\n        parent_beta_arr.append(parent_beta)\n        # Store this iteration's empirical median beta.\n        empirical_beta_arr.append(median)\n        # Increment the iteration counter.\n        i += 1\n    if beta_diff < similarity_tolerance:\n        print(\"Accepted parent beta: %.3f, Empirical beta: %.3f;\\nAbs. Difference: %.3f\" % (parent_beta,median,beta_diff))\n    else:\n        print(\"No suitable parent beta found in %u iterations with similarity tolerance %.3f;\\nIncrease max_iter to fix.\" % (max_iter,similarity_tolerance))\n        betas = None\n    return parent_beta_arr,empirical_beta_arr,betas"]},{"cell_type":"code","execution_count":1,"id":"dcdef5bb-e5fe-4017-96a3-b86eb7ffc225","metadata":{},"outputs":[],"source":["import matplotlib as mpl\n\n# Check if a previous run has already produced betas data for the active processing pathway and only perform the analysis if not.\nif not os.path.exists(file_prepend+\"-betas.json\") or FORCE_OVERWRITE:\n    # Force reload from .csv file so that list processing is standardised.\n    df = pd.read_csv(file_prepend + \"-summary.csv\",index_col=0)\n\n    # Look at only sections that have an alteration index of 1 (heavy) or 0 (partly).\n    df = df[(df[\"alteration\"]==1) | (df[\"alteration\"]==0)]\n\n    # Load curve fit data.\n    curve_fits = np.array(json.loads(\"[\" + \",\".join(df[\"curve_fit\"]) + \"]\"))\n\n    # Define data structure in which betas data can be stored.\n    betas_data = dict()\n    # Iterate through valid samples.\n    for i,sample in enumerate(df.index):\n        # Extract observed values derived from data.\n        alpha = curve_fits[i][0]\n        observed_beta = curve_fits[i][1]\n        n = df[\"n\"].iloc[i]\n\n        # Determine beta values after \"fit\" attempt.\n        parent_beta_arr,empirical_beta_arr,betas = parent_beta_search(observed_beta,alpha,n,similarity_tolerance=0.5)\n\n        if betas is not None:\n            # Colormap for use in plotting iterations throughout the \"fit\" attempt.\n            cmap = \"viridis\"\n            # Describe plot layout.\n            fig,axs = plt.subplots(1,2,constrained_layout=True,figsize=(16,10))\n            # Plot the observed empirical beta\n            axs[0].axhline(observed_beta,label=\"Data $\\\\beta$\",color=\"lightgreen\",linestyle=\"-.\")\n            # Scatter parent vs resultant empirical beta values.\n            scattered = axs[0].scatter(parent_beta_arr,empirical_beta_arr,c=range(len(parent_beta_arr)),cmap=cmap)\n            # Set axes labels.\n            axs[0].set_xlabel(\"Parent $\\\\beta$\")\n            axs[0].set_ylabel(\"Resultant Empirical $\\\\beta$\")\n            # Title subplot.\n            axs[0].set_title(\"Relation between parent and resultant $\\\\beta$\")\n            # Display colorbar.\n            cb = fig.colorbar(scattered,ax=axs[0])\n            cb.ax.set_title(\"Iteration\")\n\n            # Plot the distributions of observed betas at different sample sizes for this iteration (for a constant parent beta).\n            boxplot_dict = axs[1].boxplot(betas,positions=[n],patch_artist=True,boxprops={\"facecolor\":mpl.colormaps[cmap](255)})\n            # Plot the observed empirical beta\n            axs[1].axhline(observed_beta,label=\"Data $\\\\beta$\",color=\"lightgreen\",linestyle=\"-.\")\n            # Plot the parent beta line for the final/matching case.\n            parent_beta = parent_beta_arr[-1]\n            axs[1].axhline(parent_beta,label=\"Parent $\\\\beta$\",color=\"lightblue\",linestyle=\"--\")\n            # Set y label.\n            axs[1].set_ylabel(\"$\\\\beta$\")\n            # Display legend.\n            axs[1].legend()\n            # Title subplot.\n            axs[1].set_title(\"Distribution of $\\\\beta$ for \\\"best fit\\\" values\")\n            # Hide x axis.\n            axs[1].get_xaxis().set_visible(False)\n\n            # Title overall plot.\n            fig.suptitle(f\"{sample}; $\\\\alpha$: {alpha}, $n$: {n}\")\n            plt.savefig(os.path.join(\".\",\"imgs\",f\"{file_prepend}-beta-fitting-{sample}.png\"))\n\n            # 1.5 IQR more below box\n            lowest_whisker = boxplot_dict[\"whiskers\"][0].get_ydata()[1]\n            # 1.5 IQR more above box\n            highest_whisker = boxplot_dict[\"whiskers\"][1].get_ydata()[1]\n            betas_data[sample] = {\"1.5IQR_below\":lowest_whisker,\n                                  \"1.5IQR_above\":highest_whisker,\n                                  \"parent_beta\":parent_beta,\n                                  \"median_beta\":np.median(betas),\n                                  \"true_beta\":observed_beta}\n    # Save betas data.\n    with open(file_prepend+\"-betas.json\",\"w\") as outfile:\n        json.dump(betas_data,outfile)\n    plt.show()"]},{"cell_type":"markdown","id":"9e332fb8-a0cd-4222-89d5-7658975c7d0d","metadata":{},"source":["<table>\n<tr>\n<th><img src=\"./imgs/modified-10-beta-fitting-06C.png\"></th>\n<th><img src=\"./imgs/modified-10-beta-fitting-07A.png\"></th>\n<th><img src=\"./imgs/modified-10-beta-fitting-M01.png\"></th>\n</tr>\n<th><img src=\"./imgs/modified-10-beta-fitting-M04.png\"></th>\n<th><img src=\"./imgs/modified-10-beta-fitting-M08.png\"></th>\n<th></th>\n<tr>\n</tr>\n<tr>\n<th><img src=\"./imgs/modified-10-beta-fitting-M07B1.png\"></th>\n<th><img src=\"./imgs/modified-10-beta-fitting-M07B2.png\"></th>\n<th></th>\n</tr>\n</table>\n\nPlotting these uncertainties with key:\n\n-   Where the uncertainty here is defined as the range between 2 interquartile ranges from the median for the collection (with default size 1000) of &beta; values generated by random subsampling.\n\n![img](./imgs/betas-key.png)\n\n"]},{"cell_type":"code","execution_count":1,"id":"7b029fc2-d2e9-4ffb-b4df-b751a09772ba","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"None"}],"source":["# Load betas data.\nwith open(file_prepend+\"-betas.json\") as infile:\n    betas_data = json.load(infile)\n\n# Iterate through samples of potential interest.\nfor sample,alteration in alteration_degree.items():\n    # Check if sample is of a degree of alteration that can be compared.\n    if alteration in [0,1]:\n        # Load betas data for the sample.\n        sample_betas_data = betas_data[sample]\n        # Plot \"uncertainty\" bars.\n        plt.plot([alteration]*2,\n                 [sample_betas_data[\"1.5IQR_\"+k] for k in [\"above\",\"below\"]],\n                 c=\"k\",\n                 alpha=0.5,\n                 marker=\"_\",\n                 zorder=0)\n        # Plot the parent beta and the median beta derived from random sampling using that parent beta.\n        plt.scatter([alteration]*2,\n                    [sample_betas_data[k] for k in [\"parent_beta\",\"median_beta\"]],\n                    c=[\"red\",\"blue\"])\n        # Plot the observed beta derived from real data.\n        plt.scatter(alteration,\n                    sample_betas_data[\"true_beta\"],\n                    marker=\"+\",\n                    c=\"yellow\")\n        # Labelling each sample.\n        plt.text(alteration+0.01,\n                 sample_betas_data[\"true_beta\"],\n                 sample,\n                 va=\"center\")\n        # Label the x axis with the degree of alteration represented by plotted samples.\n        plt.xlabel(\"Degree of alteration\")\n        plt.gca().set_xticks([0,1],[\"medium\",\"high\"])\n        # Label y axis.\n        plt.ylabel(\"$\\\\beta$ (aka $b$)\")\n        # Title plot.\n        plt.title(\"$\\\\beta$ (aka $b$) parameter values\")\nplt.show()"]},{"cell_type":"markdown","id":"a90715f2-bb0d-44af-bc31-fbbc53fc255e","metadata":{},"source":["The large uncertainties confirms the previous suggestion that the uncertainties are significant - hence the differences in &beta; between partly and heavily altered samples are not robust.\n\n"]},{"cell_type":"markdown","id":"352762e7-f4b8-4de2-a1c7-b34d9c65d99a","metadata":{},"source":["## Conclusions\n\n"]},{"cell_type":"markdown","id":"0ad91dee-3840-4ddb-b665-accbe8427d5c","metadata":{},"source":["This refined analysis demonstrates that many of the previous observations and inferences are not robust. The only observations suitable for further interpretation are:\n\n1.  The semilog grain size area distributions have a negative exponential shape.\n2.  The shape of the reflector area vs nearest neighbour plots are also mostly independent of the processing pathway, showing a broad range of nearest-neighbour distances for small grains, and a narrower range/convergence around 0.2 mm<sup>2</sup> as grain size increases.\n\nOn the data processing side, the differences between results produced by the original method and this new method also highlights the importance of correctly identifying and merging grains that should appear as one, which is a time consuming process (either by doing so manually or developing an automated method).\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}
